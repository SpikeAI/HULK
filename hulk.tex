%!TEX root = BoutinRuffierPerrinet17spars_poster.tex
%!TeX TS-program = Lualatex
%!TeX encoding = UTF-8 Unicode
%!TeX spellcheck = en-US
%!BIB TS-program = bibtex
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Draft{1}%
\def\Draft{0}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%                    METADATA
% --------------------------------------------------------------------------
\newcommand{\AuthorA}{Boutin}%
\newcommand{\FirstNameA}{Victor}%
\newcommand{\AuthorB}{Franciosini}%
\newcommand{\FirstNameB}{Angelo}%
\newcommand{\AuthorC}{Perrinet}%
\newcommand{\FirstNameC}{Laurent U}%
\newcommand{\Institute}{Institut Neurosciences Timone, Marseille, France}%\\ Aix Marseille Univ, CNRS}%
\newcommand{\Organism}{Aix Marseille Universit\'e, CNRS}%
\newcommand{\Address}{27, Bd. Jean Moulin, 13385 Marseille Cedex 5, France}%
\newcommand{\Website}{http://invibe.net/LaurentPerrinet}%
\newcommand{\EmailA}{victor.boutin@univ-amu.fr}%
\newcommand{\EmailB}{angelo.franciosini@univ-amu.fr}%
\newcommand{\EmailC}{laurent.perrinet@univ-amu.fr}%victor.boutin@univ-amu.fr
\newcommand{\Title}{%
A fast algorithm for unsupervised learning
%alternatives: - Homeostasis is necessary for the unsupervised learning of orientation-selective cells
}%
\newcommand{\Abstract}{
The formation of structure in the brain, that is, of the connection
between cells within neural populations, is a largely unsupervised
learning process, that is, that the emergence of this architecture is
mostly self-organized. In the primary visual cortex of mammals, for
example, one may observe during development the emergence of cells
selective to localized, oriented features. This lead to the development
of a rough representation of contours of the retinal image in area V1. A
major difficulty in defining unsupervised learning algorithms is that
during this process, on the one hand the coding is performed knowing an
immature structure and on the other hand, the adaptation of this
structure is carried out knowing a code that is not yet optimal. We
propose here a fast algorithm compatible with a neuromimetic
architecture which solves this problem and allows for the fast emergence
of localized filters sensitive to orientation. The key to this algorithm
lies in a simple yet optimal mechanism of homeostasis that reconciles
the antagonistic processes that occur at coding and learning time scale.
We tested this unsupervised algorithm with this homeostasis rule for a
range of existing unsupervised learning algorithms coupled with
different neural coding algorithms. In addition, we propose a
simplification of this optimal homeostasis rule by implementing a simple
heuristic on the probability of activation of neurons. Compared to the
optimal homeostasis rule, we show that this heuristic allows to
implement a faster unsupervised learning algorithm while keeping a large
part of its effectiveness. Finally, we show that such an algorithm can
be extended to convolutional architectures and we show the results
obtained on different natural image databases. These results demonstrate
the potential application of such a strategy to the fast classification
of images, for example in hierarchical and dynamic architectures.
%
%One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any sample as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that sparse coding can be optimized by designing a proper homeostatic rule regulating the competition between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalisation similar to what is observed in biological neural networks. We validated this theoretical insight by challenging the matching pursuit sparse coding algorithm with the same learning rule but with or without homeostasis. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding accuracy did not vary much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we will apply this algorithm to the optimisation of a visual system to be embedded in an aerial robot. In summary, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms. 
 %The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. 
}
\newcommand{\Keywords}{Retina, Sparseness, Computer vision, Unsupervised learning, Neuroscience}%
\newcommand{\Acknowledgments}{%
This work was supported by XXX and the Doc2Amu project which received funding from a co-fund with the European Union's Horizon 2020 research and innovation programme and the region Provence Alpes Cote d'Azur. }
\newcommand{\Links}{%
\begin{itemize}
\item Correspondence and requests for materials should be addressed to LUP (email:\EmailC ). 

\item Code and supplementary material available at \url{\WebsiteC/Publications/BoutinFranciosiniPerrinet18hulk}.
\end{itemize}
} %



\subsection{Introduction}\label{introduction}

Neural architecture is a complex dynamic system that operates at
different time scales. In particular, one of its properties is to
succeed in the feat of being able to both represent information quickly
but also to be able to adapt in the long term autonomously
(self-organized) to optimize this coding. In the case of the mammalian
primary visual cortex (V1) for instance, one can observe the rapid
coding of retinal image, as a process of transforming the visual
information into a rough sketch that represents the outlines of objects
in the image. This rapid operation, of the order of 50 milli-seconds in
humans, is key to the results of Hubel and Wiesel, who showed that some
mammalian primary visual cortex cells have relatively localized
receptive fields which are predominantly selective at different
orientations. A major step in understanding this observation has been to
show that the emergence of these filters can be explained as the
coupling of a simple Hebbian learning with an optimal coding of the
image. Indeed, the work of Bruno Olshausen has shown that by imposing a
sparse encoding of the image, we can obtain the emergence of such cells
in a neural-type model. This type of unsupervised learning algorithm has
been related to many other types of optimal representation algorithms
used in both signal processing and artificial intelligence. In
particular, this algorithm allows the extraction of the independent
components of the signal, for example the orientations in the image.
This representation makes it possible to observe the emergence of
representations which are invariant to geometric transforms such as
rotations and translations in V1. A recent rapprochement between these
algorithms and machine learning algorithms makes it possible to place
them in a new perspective. Indeed, these unsupervised algorithms are
equivalent to a gradient descent optimization over an informational-type
coding cost. This cost makes it possible to quantitatively evaluate the
exploration of new components in the signal for learning with respect to
the exploitation of these components in the coding. As such, this remark
shows us that unsupervised learning consists of two antagonistic
mechanisms, a long time scale that corresponds to the learning and
exploration of new components and a faster scale that corresponds to
coding.

A component often ignored in this type of learning is the set of
mechanisms of homeostasis. Indeed, these are implemented in the original
algorithms (Olshausen, Rehn, \ldots{}) as a heuristic that simply
prevents the algorithm from diverging. It consists in some recent
algorithms to constitute only of an equalization of the energy of each
component of the dictionary (Mairal). However, the neural mechanisms of
homeostasis are at work in many components of the neural code and are
essential to the overall functioning of the neural code. For example,
the networks of GABA-type inhibitory neurons make it possible to
regulate the overall activity of neural populations. This mechanism then
makes it possible to balance the contribution of the excitatory
populations with respect to that in inhibitory populations. By this
mechanism, this type of so-called balanced networks can explain many
features inherent in the primary visual cortex (Hansel). These
mechanisms, which often take the form of normalization rules (Schwartz)
in the network, are often used as normative theory to explain the
mechanisms present in the primary visual cortex (see Heeger). However,
this work is often intended to show that the cells that emerge from
these algorithms have the same characteristics as that observed in
neurophysiology (Rehn, Loxley). Other algorithms use nonlinearities that
implicitly implement homeostatic rules in neuromimic algorithms
(Gerstner and Brito). These non-linearities are mainly used in the
output of successive layers of deep learning networks that are nowadays
widely used for image classification or artificial intelligence. However
most of these non-linear normalization rules are based on heuristics
mimicking neural mechanisms but are not justified as part of the global
problem of unsupervised learning.

It is important to note at this point that learning algorithms used in
artificial intelligence can shed new light on the functioning of these
neural processes and in particular on unsupervised learning. In the
field of machine learning, unsupervised learning corresponds to learning
representation dictionaries when the categorization data is unknown.
This training is therefore carried out autonomously and is particularly
used for image and signal compression, object detection, source
separation and noise reduction in raw signals. As such, this class of
algorithms is extremely useful in the first layers of artificial
intelligence algorithms such as deep algorithms. There are many variants
of such algorithms that take the form of either an optimization of
information transfer (Bell), or in the form of learning rules in deep
learning or recursive algorithms in statistics and probabilities with,
for example, projection continuation. An important class of these
algorithms considers that all the solutions that will be considered are
those that correspond to an optimal coding. These solutions take the
form of parsimonious coding, i. e. for which a small number of
components will be selected according to the size of the dictionary.
This principle is general enough to be applied to many signal classes
and allow a mathematical analysis of this problem (Donoho). In
particular, we have shown above that a rapid algorithm of parsimonious
coding can be implemented in a normal neural architecture (Perrinet,
2010). It has also been shown that such coding can improve
classification algorithms, in particular by limiting the number of
layers required in a deep learning algorithm for classifying images that
contain or do not contain animals (Perrinet and Bednar, 2015). However,
recent studies seem to question this principle of parsimonious coding
(Eichhorn 2009, Zoran and Weiss 2012) and suggest that simpler analysis
applied in a complex metric is sufficient to explain the emergence of
selective filters with orientation similar to those observed in the
primary visual cortex.

In order to offer a broader perspective on this problem, we will try to
express it in the generic form of a probabilistic problem. This approach
is already widely used in Barlow's early work under the term of
redundancy reduction principle (see also Atick). It led to translating
this learning problem into a problem of efficient coding, for example by
implementing inhibition rules in the retinal receptive field of the
saber-toothed tiger (??? Srinivasan, 1981). Other studies show that
these rules are tantamount to forcing the system to be close to a
criticality regime and optimizing the balance between coding and
learning optimization (Beggo, 2008 in Sandin) More generally, we will
place ourselves in the framework of the principle of free energy
minimization formulated by Karl Friston. This principle makes it
possible to explicitly address the problem of coding and unsupervised
learning, also during learning. In this theory, learning is no longer a
goal in itself but contributes to the the minimization of free energy at
different time scales, from coding to learning but also to the
intermediate time of homeostasis. According to this principle, the
overall goal of neural system is to be able to best predict any sensory
input. This principle results in changes in the structure of the
population (synaptic connections) but also in adaptation rule before the
convergence of the learning. The goal of these processes is thus simply
to not be surprised by the sensory input. As such, the aim of learning
is to obtain a coding that is the least variable a priori, that is to
say, before having received the sensory information. On the one hand,
this theory extends that of Olshausen and shows that homeostasis also
has a predictive part. Thus, this will also allow us to formulate a
normative theory of the neural code at the time scale of homeostasis.
This allows us also to associate homeostasis with adaptation mechanisms
(Rao Balard). Thus, the set of processes at different time scales are
thus considered as working synergestically and provide for a novel
normative theory of coding in early sensory areas such as V1.

This paper is organized according to the following plan. First we will
show the importance of homeostasis in unsupervised learning algorithms.
We will derive an optimal rule for homeostatic adaptation based on
histogram equalization. We will then show quantitative results of this
optimal algorithm by applying it to different pairs of coding and
learning algorithms. By using different learning databases, we will be
able to give a quantitative analysis that will make it possible to
compare these different solutions. To simplify the optimal rule of
homeostasis, we will then deliver a neuro-mimetic homeostasis algorithm
derived from the optimal rule by using a simple heuristic. We will then
compare the results of this new algorithm with the optimal algorithm as
well as with other existing unsupervised learning algorithms (Olshausen,
Sandin). Moreover, by its nature, this algorithm can easily be extended
to convolutional networks such as those used in deep learning neural
networks. This extension is possible by extending the filter dictionary
by the hypothesis of invariances to the translation of representations.
Our results on different databases show the stable and rapid emergence
of characteristic filters on these different bases. This result shows a
probable prospect of extending this representation and for which we hope
to obtain classification results superior to the algorithms existing in
the state-of-the-art. Finally we will conclude by showing the importance
of homeostasis in unsupervised learning algorithms. By developing this
fast learning algorithm we hope for its rapid application in artificial
intelligence algorithms. This type of architecture is economical,
efficient and fast. It makes it possible to be transferred to most deep
learning algorithms, a major flaw of which is to be very greedy in
computing resources. In particular, we are considering perspectives for
coding within a dynamic flow of sensory data and we hope to apply this
type of algorithm on embedded systems such as aerial robots. Along with
this, we hope that this new type of rapid unsupervised learning
algorithm can provide a normative theory for the coding of information
in low-level sensory processing, whether it is visual or auditory, for
example.

\subsection{Algorithm}\label{algorithm}}


%https://blog.keras.io/building-autoencoders-in-keras.html Otherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data. 

% LUP\ IS\ HERE\ 

Le principe de l'algorithme consiste à rajouter des variables latentes
qui vont permettre de prédire les déviations dans le codage ou
l'apprentissage pendant l'apprentissage.

\hypertarget{algorithm}{%
\subsection{en: Algorithm}\label{algorithm}}

\hypertarget{muxe9thode}{%
\subsection{Méthode}\label{muxe9thode}}


\subsection{Discussion and conclusion}\label{discussion-et-conclusion}
