%!TEX root = hulk.tex
%!TeX TS-program = pdflatex
%!TeX encoding = UTF-8 Unicode
%!TeX spellcheck = en-US
%!BIB TS-program = biber
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
% https://github.com/bicv/Perrinet2015BICV_sparse/blob/master/sparse.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Draft{0}%
\def\Draft{1}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\if 1\Draft
\documentclass[a4paper, 11pt, draft]{article} % For LaTeX2e
\else
\documentclass[a4paper, 11pt]{article} % For LaTeX2e
\fi
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2018}
%\usepackage{nips_2018}
%\usepackage[final]{nips_2018}
%
%============ common ===================
%\usepackage[utf8]{luainputenc}
%\usepackage[english]{babel}%
%%\usepackage{csquotes}%
%\usepackage[autostyle]{csquotes}
%%% Sans-serif Arial-like fonts
%\renewcommand{\rmdefault}{phv} 
%\renewcommand{\sfdefault}{phv} 
%     \usepackage{textcomp}
%     \usepackage{libertine}%[sb]
%     \usepackage[varqu,varl]{inconsolata}% sans serif typewriter
%     \usepackage[libertine,bigdelims,vvarbb]{newtxmath} % bb from STIX
%     \usepackage[cal=boondoxo]{mathalfa} % mathcal
%%     \useosf % osf for text, not math
%     \usepackage[supstfm=libertinesups,%
%       supscaled=1.2,%
%       raised=-.13em]{superiors}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{dsfont}
\usepackage{hyperref}       % hyperlinks
% --------------------------------------------------------------------------
%                    METADATA
% --------------------------------------------------------------------------
\newcommand{\AuthorA}{Boutin}%
\newcommand{\FirstNameA}{Victor}%
\newcommand{\AuthorB}{Franciosini}%
\newcommand{\FirstNameB}{Angelo}%
\newcommand{\AuthorC}{Perrinet}%
\newcommand{\FirstNameC}{Laurent U}%
\newcommand{\Institute}{Institut Neurosciences Timone, Marseille, France}%\\ Aix Marseille Univ, CNRS}%
\newcommand{\Organism}{Aix Marseille Universit\'e, CNRS}%
\newcommand{\Address}{27, Bd. Jean Moulin, 13385 Marseille Cedex 5, France}%
\newcommand{\Website}{http://invibe.net/LaurentPerrinet}%
\newcommand{\EmailA}{victor.boutin@univ-amu.fr}%
\newcommand{\EmailB}{angelo.franciosini@univ-amu.fr}%
\newcommand{\EmailC}{laurent.perrinet@univ-amu.fr}%victor.boutin@univ-amu.fr
\newcommand{\WebsiteC}{http://invibe.net/LaurentPerrinet}%
%Submission Format
%
%Before you log onto the submission website, you should have the following items prepared. Submissions that do not meet these guidelines may be rejected. Abstract selection is a competitive process, so please read carefully:
%
%    * Title - 100 characters or fewer (including spaces), capitalized in sentence case
%    * Author list - including email addresses of all authors
%    * Summary - 300 words or fewer, text only. This summary will appear in the printed conference program if your submission is accepted.
%    * PDF submission - This should be one page (A4 or US Letter) in PDF format. The title and author names should appear at the head of the page (contact info may be omitted), followed by the 300-word Summary. The rest of the document should expand upon the central question(s), approach, results, and/or conclusions of the study. Figures are optional. You may include equations as appropriate. Font size (including any figure legends) must be at least 12 point. Margins should be at least 0.5". Because space is limited, you do not need to touch upon all the major points of the Summary; rather you should aim to add whatever detail is need to help reviewers evaluate the technical correctness and significance of your study.
%    * Two Program Committee members (see below) who could handle your submission. We cannot honor all specific requests, but your list will help us direct your submission to Program Committee members with appropriate expertise. 
%
%Evaluation Criteria
%
%PDF submissions will be evaluated on the basis of the following criteria (equally weighted):
%
%    * Significance, originality, and potential impact of your claims to the COSYNE audience. Please indicate why your question is important and how your claims advance the field. Reviewers will be asked to rate your claims in this regard, without regard to whether your methods/results technically support those claims (see below). If your aim is to develop a new approach or technique, indicate why this is useful. Implicitly included in this category is the quality of fit of your abstract for the COSYNE conference audience -- potentially inappropriate abstracts include pure machine learning studies, or studies of single cells with no clear implications for neural systems.
%    * Technical correctness. Reviewers will be asked to rate the quality of your methods and/or data in supporting your claims (see above). Thus, you should clearly outline the methods used, results obtained, and critical controls. Include sufficient detail in your submission to allow reviewers to evaluate whether your approach is appropriate for supporting your claims, and whether your results are technically sound. However, your submission should not be as detailed as the Methods section of a full-length paper (e.g., do not list concentrations of drugs, etc.). 
%
%You should not feel obliged to fill the entire page, and we expect that many successful submissions will not include figures.
%
%Approximately 20 submissions will be chosen for short talks and ~300 will be chosen for poster presentations. We expect ~20% of submissions will be rejected due to limitations on poster space. 
\newcommand{\Title}{%
A fast algorithm for unsupervised learning
%alternatives: - Homeostasis is necessary for the unsupervised learning of orientation-selective cells
%# Homeostatic Unsupervised Learning of Kernels
}%
\newcommand{\Abstract}{
The formation of structure in the brain, that is, of the connection between cells within neural populations, is by large an unsupervised learning process: The emergence of this architecture is mostly self-organized. In the primary visual cortex of mammals, for example, one may observe during development the emergence of cells selective to localized, oriented features. This leads to the development of a rough representation of contours of the retinal image in area V1. A major difficulty in defining unsupervised learning algorithms is that during this process, on the one hand the coding is performed knowing an immature structure and on the other hand, the adaptation of this structure is carried out knowing a code that is not yet optimal. We propose here a fast algorithm compatible with a neuromimetic architecture which solves this problem and allows for the fast emergence of localized filters sensitive to orientation. The key to this algorithm lies in a simple yet optimal mechanism of homeostasis that reconciles the antagonistic processes that occur at the coding and learning time scales. We tested this unsupervised algorithm with this homeostasis rule for a range of existing unsupervised learning algorithms coupled with different neural coding algorithms. In addition, we propose a simplification of this optimal homeostasis rule by implementing a simple heuristic on the probability of activation of neurones. Compared to the optimal homeostasis rule, we show that this heuristic allows to implement an even faster unsupervised learning algorithm while keeping a large part of its effectiveness. These results demonstrate the potential application of such a strategy to the fast classification of images, for example in hierarchical and dynamic architectures.
}
\newcommand{\Keywords}{Vision, Sparseness, Computer vision, Unsupervised learning, Neuroscience}%
\newcommand{\Acknowledgments}{%
This work was supported by XXX and the Doc2Amu project which received funding from a co-fund with the European Union's Horizon 2020 research and innovation programme and the region Provence Alpes Cote d'Azur. }
\newcommand{\Links}{%
\begin{itemize}
\item Correspondence and requests for materials should be addressed to LUP (email:\EmailC ). 
\item Code and supplementary material available at \url{\WebsiteC/Publications/BoutinFranciosiniPerrinet18hulk}.
\end{itemize}
} %
% --------------------------------------------------------------------------
\usepackage{filecontents}

\begin{filecontents}{hulk.bib}

@ARTICLE{Kingma2013,
   author = {{Kingma}, D.~P and {Welling}, M.},
    title = "{Auto-Encoding Variational Bayes}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6114},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Doersch2016,
	Abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	Author = {Doersch, Carl},
	Journal = {arXiv:1606.05908 [cs, stat]},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Note = {00083 arXiv: 1606.05908},
	Title = {Tutorial on {Variational} {Autoencoders}},
	Url = {http://arxiv.org/abs/1606.05908},
	Urldate = {2018-05-11},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.05908}}
	
@article{Ringach02,
	Author = {Dario L. Ringach},
	Date-Added = {2017-10-19 10:17:41 +0000},
	Date-Modified = {2017-11-20 21:01:37 +0000},
	Journal = {Neurophysiol},
	Title = {Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QHy4uLy4uLy4uL0Rvd25sb2Fkcy80NTUuZnVsbC5wZGbSFwsYGVdOUy5kYXRhTxEBVgAAAAABVgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DDQ1NS5mdWxsLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwACAAAKIGN1AAAAAAAAAAAAAAAAAAlEb3dubG9hZHMAAAIAMC86VXNlcnM6YW5nZWxvZnJhbmNpb3Npbmk6RG93bmxvYWRzOjQ1NS5mdWxsLnBkZgAOABoADAA0ADUANQAuAGYAdQBsAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC5Vc2Vycy9hbmdlbG9mcmFuY2lvc2luaS9Eb3dubG9hZHMvNDU1LmZ1bGwucGRmABMAAS8AABUAAgAY//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AsAC1AL0CFwIZAh4CKQIyAkACRAJLAlQCWQJmAmkCewJ+AoMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAChQ==}}

\end{filecontents}
\usepackage[natbib=true,
			style=numeric-comp, %ieee, %
      		sortcites=true,
			abbreviate=true,
			maxcitenames=2,
			maxnames = 5,
			firstinits=true,
			uniquename=init,
			sorting=none,
			doi=false,
			url=true,
			isbn=false,
			eprint=false,
			texencoding=utf8,
			bibencoding=latin1,
			%autocite=superscript,
			backend=biber,
			%articletitle=false,
			]{biblatex}%
\AtEveryBibitem{
  \clearfield{month}
  \clearfield{day}
  \clearfield{url}
  \clearfield{note}
  \clearfield{comment}
%  \clearfield{edition}
%  \clearfield{publisher}
}
\addbibresource{hulk.bib}%
\addbibresource{https://raw.githubusercontent.com/bicv/Perrinet2015BICV_sparse/master/biblio_sparse.bib}%
%\newcommand{\citep}[1]{(\cite{#1})}
%\newcommand{\citet}[1]{(\textcite{#1})}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[unicode,linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue,pdfborder={0 0 0}]{hyperref}%
%\hypersetup{%
%pdftitle={\Title},%
%pdfauthor={Corrresponding author: \AuthorC < \EmailC > \Address - \WebsiteC },%
%pdfkeywords={\Keywords},%
%pdfsubject={\Acknowledgments}%
%}%
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[dvipsnames]{xcolor}

\usepackage{tikz}%
%\if 1\Draft
%\usepackage{setspace}
%\fi
% MATHS (AMS)
%\usepackage{amsmath}
%%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{amsfonts, amssymb, amscd}
%: symbols
\newcommand{\umin}[1]{\underset{#1}{\min}\;}
\newcommand{\enscond}[2]{\lbrace #1, #2 \rbrace}
\newcommand{\norm}[1]{|\!| #1 |\!|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
\newcommand{\eqdef}{\ensuremath{\stackrel{\mbox{\upshape\tiny def.}}{=}}}
\newcommand{\eqset}{\ensuremath{\stackrel{\mbox{\upshape\tiny set}}{=}}}
\newcommand{\eq}[1]{\begin{equation*}#1\end{equation*}}
\newcommand{\eql}[1]{\begin{equation}#1\end{equation}}
\newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
\newcommand{\NN}{\mathbb{N}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\CC}{\mathbb{C}}
\usepackage{siunitx}
%\renewcommand{\cite}{\citep}%
\newcommand{\ms}{\si{\milli\second}}%
\newcommand{\m}{\si{\meter}}%
\newcommand{\s}{\si{\second}}%
\newcommand{\seeFig}[1]{Figure~\ref{fig:#1}}%
\newcommand{\seeSec}[1]{Section~\ref{sec:#1}}%
\newcommand{\seeEq}[1]{Eq.~\ref{eq:#1}}%

\newcommand{\coef}{\mathbf{a}} % image's hidden param
\newcommand{\image}{\mathbf{I}} % the image
\newcommand{\dico}{\Phi} % the dictionary
%\showthe\columnwidth
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
%\usepackage{titling}
%\setlength{\droptitle}{-6em}
%\posttitle{\par\end{center}\vspace{-2.3em}}

\title{\Title}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{%
%\makebox[.45\linewidth]{\FirstNameA\ \AuthorA}
%\and \makebox[.45\linewidth]{\FirstNameB\ \AuthorB}% \\ \Institute\ \Organism
%\makebox[.9\linewidth]{\FirstNameA\ \AuthorA and \FirstNameB\ \AuthorB} %\\ \Institute\
\FirstNameA\ \AuthorA \and \FirstNameB\ \AuthorB \and \FirstNameC\ \AuthorC
}
\date{\Institute\ \\ 
\Organism\
}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

%\author{
%David S.~Hippocampus\thanks{Use footnote for providing further
%information about author (webpage, alternative
%address)---\emph{not} for acknowledging funding agencies.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213 \\
%\texttt{hippo@cs.cranberry-lemon.edu} \\
%% examples of more authors
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \AND
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%}

%\setlength{\pdfpagewidth}{\paperwidth}
%\setlength{\pdfpageheight}{\paperheight}
\usepackage{times}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}


%\pagestyle{empty}		% No page numbers
\begin{document}
%
% make the title area
\maketitle
%\vspace*{-.6cm}
\begin{abstract}
%{\bf Abstract.}
\Abstract
%\vspace*{-.4cm}
\end{abstract}
\thispagestyle{empty}
\section{Introduction}\label{introduction}
%: motivation: why sparseness? from data to knowledge
The neural architecture is a complex dynamic system that operates at different time scales. In particular, one of its properties is to succeed in the feat of being able to both represent information quickly but also to be able to adapt in the long term autonomously (self-organized) to optimize this coding. In the case of the mammalian primary visual cortex (V1) for instance, one can observe the rapid coding of retinal image, as a process of transforming the visual information into a rough sketch that represents the outlines of objects in the image. This rapid operation, of the order of 50 milli-seconds in humans, is key to the results of~\citet{Hubel68}, who showed that some cells of the mammalian primary visual cortex have relatively localized receptive fields which are predominantly selective at different orientations. A major step in understanding this observation has been to show that the emergence of these filters can be explained as the coupling of a simple Hebbian learning with an ``optimal'' coding of the image. Indeed, the work of~\citet{Olshausen96} has shown that by imposing a sparse encoding of the image, we can obtain the emergence of such cells in a neural-type model. This type of unsupervised learning algorithm~\citep{Olshausen97} has been related to many other types of optimal representation algorithms used in both signal processing and artificial intelligence. In particular, this algorithm allows the extraction of the independent components of the signal, for example the orientations in the image. This representation makes it possible to observe the emergence of representations which are invariant to geometric transforms such as rotations and translations in V1. A recent rapprochement between these algorithms and machine learning algorithms makes it possible to place them in a new perspective. Indeed, these unsupervised algorithms are equivalent to a gradient descent optimization over an informational-type coding cost~\citep{Elad10}, VAE). This cost makes it possible to quantitatively evaluate the exploration of new components in the signal for learning with respect to the exploitation of these components in the coding. As such, this remark shows us that unsupervised learning consists of two antagonistic mechanisms, a long time scale that corresponds to the learning and exploration of new components and a faster scale that corresponds to coding.

%: in machine learning
It is important to note at this point that learning algorithms used in artificial intelligence can shed new light on the functioning of these neural processes and in particular on unsupervised learning. In the field of machine learning, unsupervised learning corresponds to learning representation dictionaries when the categorization data is unknown. This training is therefore carried out autonomously and is particularly used for image and signal compression, object detection, source separation and noise reduction in raw signals. As such, this class of algorithms is extremely useful in the first layers of artificial intelligence algorithms such as deep learning algorithms. There are many variants of such algorithms that take the form of either an optimization of information transfer (Bell), or in the form of recursive algorithms in statistics and probabilities with, for example, projection continuation. An important class of these algorithms considers that all the solutions that will be considered are those that correspond to an optimal coding. These solutions take the form of parsimonious coding, i. e. for which a small number of components will be selected according to the size of the dictionary. This principle is general enough to be applied to many signal classes and allow a mathematical analysis of this problem (Donoho). In particular, we have shown above that a rapid algorithm of parsimonious coding can be implemented in a normal neural architecture~\citep{Perrinet10shl}. It has also been shown that such coding can improve classification algorithms, in particular by limiting the number of layers required in a deep learning algorithm for classifying images that contain or do not contain animals~\citep{PerrinetBednar15}. However, recent studies seem to question this principle of parsimonious coding (Eichhorn 2009, Zoran and Weiss 2012) and suggest that simpler analysis applied in a complex metric is sufficient to explain the emergence of selective filters with orientation similar to those observed in the primary visual cortex.

%: encompassed in probabilistic approaches
In order to offer a broader perspective on this problem, we will try to express it in the generic form of a probabilistic problem. This approach is already widely used in Barlow's early work under the term of redundancy reduction principle (see also Atick). It led to translating this learning problem into a problem of efficient coding, for example by implementing inhibition rules in the retinal receptive field of the saber-toothed tiger (??? Srinivasan, 1981). Other studies show that these rules are tantamount to forcing the system to be close to a criticality regime and optimizing the balance between coding and learning (Beggo, 2008 in Sandin) More generally, we will place ourselves in the framework of the principle of free energy minimization formulated by Karl Friston~\citep{Friston12}. This principle makes it possible to explicitly address the problem of coding and unsupervised learning, also during learning. In this theory, learning is no longer a goal in itself but contributes to the the minimization of free energy at different time scales, from coding to learning. According to this principle, the overall goal of neural system is to be able to best predict any sensory input. This principle results in changes in the structure of the population (synaptic connections) but also in adaptation rule before the convergence of the learning. The goal of these processes is thus simply to not be surprised by the sensory input. As such, the aim of learning is to obtain a coding that is the least variable a priori, that is to say, before having received the sensory information. On the one hand, this theory extends that of Olshausen and shows that overall, sparse coding also has a predictive part. Thus, this will also allow us to formulate a normative theory of the neural code at its different time scales. Thus, the set of processes at different time scales are thus considered as working synergestically and provide for a novel normative theory of coding in early sensory areas such as V1.

%: homeostasis
%------------------------------%
%: see Figure~\ref{fig:map}
\begin{figure}%[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .9\linewidth) node {\includegraphics[width=.75\linewidth]{ssc_nohomeo}};
%\draw [anchor=north west] (0, 0) node {\includegraphics[width=.75\linewidth]{ssc_homeo}};
%4_D-comparing_convergence_results.pdf
\draw (.0\linewidth, .381\linewidth) node [above right=0mm] {$\mathsf{A}$};
\draw (.5\linewidth, .381\linewidth) node [above right=0mm] {$\mathsf{B}$};
\draw (.0\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.9\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Role of homeostasis in learning sparse representations}: 
We show the results of Sparse Hebbian Learning using  different homeostasis algorithms at convergence (20000 learning steps). For each algorithm, we show $30$ randomly drawn atoms from the $324$ filters of the same size as the image patches ($16 \times 16$) are presented in a matrix (separated by a white border). Note that their position in the matrix is arbitrary as in ICA. {\sf (A)} When switching off the cooperative homeostasis during learning, the corresponding Sparse Hebbian Learning algorithm converges to a set of filters that contains some less localized filters and some high-frequency Gabor functions that correspond to more ``textural'' features. One may wonder if these filters are inefficient and capturing noise or if they rather correspond to independent features of natural images in the LGM model. {\sf (B)} Results with the same coding and learning algorithm but by enabling homeostasis. % 
\label{fig:map}}%
\end{figure}%
%%------------------------------%
In particular, a component often ignored in this type of learning is the set of mechanisms of homeostasis. Indeed, these are implemented in the original algorithms (Olshausen, Rehn, \ldots{}) as a heuristic that simply prevents the algorithm from diverging. It consists in some recent algorithms to constitute only of an equalization of the energy of each component of the dictionary (Mairal). However, the neural mechanisms of homeostasis are at work in many components of the neural code and are essential to the overall functioning of the neural code. For example, the networks of GABA-type inhibitory neurons make it possible to regulate the overall activity of neural populations (citation needed). This mechanism then makes it possible to balance the contribution of the excitatory populations with respect to that in inhibitory populations. By this mechanism, this type of so-called balanced networks can explain many features inherent in the primary visual cortex (Hansel). These mechanisms, which often take the form of normalization rules (Schwartz) in the network, are often used as normative theory to explain the mechanisms present in the primary visual cortex (see Heeger or gain normalization by Simoncelli). However, this work is often intended to show that the cells that emerge from these algorithms have the same characteristics as that observed in neurophysiology (Rehn, Loxley). Other algorithms use nonlinearities that implicitly implement homeostatic rules in neuromimic algorithms (Gerstner and Brito). These non-linearities are mainly used in the output of successive layers of deep learning networks that are nowadays widely used for image classification or artificial intelligence. However most of these non-linear normalization rules are based on heuristics mimicking neural mechanisms but are not justified as part of the global problem of unsupervised learning. Framing this problem in a probabilistic framework allows to consider in addition to coding a	nd learning the intermediate time scale of homeostasis. This allows us also to associate homeostasis with adaptation mechanisms (Rao Balard). Our main argument is that by optimizing unsupervised learning at different time scales, we allow for the implementation of fast algorithms compatible with the performance of biological networks and in comparison with classical (Olshausen) or Deep Learning approaches.

%: outline
This paper is organized according to the following plan. First we will show the importance of homeostasis in unsupervised learning algorithms. We will derive an optimal rule for homeostatic adaptation based on histogram equalization. We will then show quantitative results of this optimal algorithm by applying it to different pairs of coding and learning algorithms. By using different learning databases, we will be able to give a quantitative analysis that will make it possible to compare these different solutions. To simplify the optimal rule of homeostasis, we will then deliver a neuro-mimetic homeostasis algorithm derived from the optimal rule by using a simple heuristic. We will then compare the results of this new algorithm with the optimal algorithm as well as with other existing unsupervised learning algorithms (Olshausen, Sandin). 
%: scripts on github
All these algorithms were implemented using using \verb+Python+ (version 3.6.5)
with packages \verb+NumPy+ (version 1.14.3) and \verb+SciPy+ (version 1.1.0)~\citep{Oliphant07}
on a cluster of Linux computing nodes.
Visualization was performed using \verb+Matplotlib+ (version 2.2.2)~\citep{Hunter07}\footnote{These python scripts are available at \url{https://github.com/XXX/ZZZ} and documented at \url{https://pythonhosted.org/ZZZ}.}. In particular, we focused in our architecture to be able to test for zillions of parameters. Indeed, we think it is better to spend time on this than to have only one model with zillions of parameters that is optimized once.
All simulations were performed 
% ----------------------------------------------------------------------
\section{Unsupervised learning and the optimal representation of images}%\label{algorithm}
% ----------------------------------------------------------------------
% - noise comes from independent mixings (Cournot) - inversely, knowing the mixes, we extract the signal
% - we wish to reduce noise as much as possible by having the least number of sources => sparse coding (inverse of central limit theorem?)

% Link with VAE : https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/


%: free-energy formulation
Visual items composing natural images are often sparse, such that knowing a model for the generation of images, the brain may use this property to reconstruct images with only a few of these items. 
In the context of the representation of natural images $\image = (\image_k)_{k=1}^L \in \RR^{L \times M}$ represented as a set of $L$ samples raveled along $M$ pixels (the $\image_{j, k} \in \RR$ are the corresponding luminance values), let us assume the generic Generative Linear Model, such that $\image = \dico \coef + \epsilon $, where by definition, the coefficients are denoted by $\coef = (\coef)_{k=1}^L \in \RR^{L \times N}$ and the dictionary $\dico \in \RR^{N \times M}$. Finally, $\epsilon  \in \RR^{L \times M}$ is without loss of generality a Normal iid noise by scaling the norm of the dictionary's rows. 
Knowing this model, unsupervised learning aims at finding the least surprising causes (the parameters $\coef$ and $\dico$) for the data  $\image$ knowing a model for the generation of this data. However, and we may only have access to a (possibly wrong) recognition model $q(\coef_k  | \image_k, \Psi)$ for any sample $k$ (where $\Psi$ are the parameters of this model) to encode the image into coefficients. In probabilistic term, this amounts to minimize the free-energy $F$ as a bound on surprise of the (unknown) density of the parameters $p( \coef_k | \image_k, \dico)$~\citep{Friston12,Kingma2013,Doersch2016}:
\begin{equation} -\log p( \coef_k | \image_k, \dico) \leq F \eqdef  KL( q(\coef_k  | \image_k, \Psi ) ||  p( \coef_k | \image_k, \dico) )   -\log p( \coef_k | \image_k, \dico) \end{equation}
where the first term in the RHS is the (positive) Kullback-Leibler distance between the density of images using the current estimate of the (unknown) marginal posterior probability. 

An advantage of this formulation is that the free energy can be rewritten as
\begin{equation} F =  KL( q(\coef_k  | \image_k, \Psi ) ||  p( \coef | \dico) )  - \int \log  p(\image_k | \coef_k, \dico ) dq(\coef_k  | \image_k, \Psi ) \end{equation}
In particular, using a known coding $\hat{\coef_k}$ of the image $\image_k$, we may approximate the free-energy by ignoring the first term in the RHS:
\begin{equation} F \approx  - \log [ p(\image | \hat{\coef_k}, \dico ) P(\hat{\coef_k}) ] = \frac{1}{2} \| \image - \dico \hat{\coef_k} \|^2  - \log p( \coef | \dico) \label{eq:sparse_cost} \end{equation}
% https://github.com/bicv/Perrinet2015BICV_sparse/blob/master/sparse.tex#L374
Such hypothesis allows to retrieve the cost that is optimized in most of existing models of unsupervised learning. Explicitly, the representation is optimized by minimizing a cost defined on prior assumptions on representation's sparseness, that is on $\log p( \coef | \dico)$. For instance, learning is accomplished in {\sc SparseNet}~\citep{Olshausen97} by defining a sparse prior probability distribution function for each coefficients in the factorial form $\log p(\coef) \sim \beta \sum_i \log ( 1 + \frac{a_i^2}{\sigma^2} )$ where $\beta$ corresponds to the steepness of the prior and $\sigma$ to its scaling (see Figure 13.2 from~\citep{Olshausen02}). Then, knowing this sparse solution, learning is defined as slowly changing the dictionary using Hebbian learning. In general, the parameterization of the prior has major impacts on results of the sparse coding and thus on the emergence of edge-like receptive fields and requires proper tuning. In fact, the definition of the prior corresponds to an objective sparseness and does not always fit to the observed probability distribution function of the coefficients. 
This is the classical formulation from~\citet{Olshausen97} but we have here highlighted thanks to the free-energy formulation that this ignores the approximation during learning (objective only valid at convergence) and that this may influence the convergence to an optimal representation.%- a nice move would be to use free-energy as a bound on surprise - and that homeostasis comes as a additional mechanism - restricts the set of possible dictionaries.

\subsection{Algorithm: Sparse Coding}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%

For instance, a L2-norm penalty term corresponds to Tikhonov regularization~\citep{Tikhonov77} or a L1-norm term corresponds to the convex cost xhich is optimized by the Lasso, LARS, Basis Pursuit, Fista or coordinate descent algorithms. 
 
More generally, thus class of optimization problems


%\begin{equation} \hat{\coef}= \Arg\Max  \end{equation}

In fact, the spiking nature of neural information demonstrates that the transition from an inactive to an active state is far more significant at the coding time scale than smooth changes of the firing rate. This is for instance perfectly illustrated by the binary nature of the neural code in the auditory cortex of rats~\citep{Deweese03}. Binary codes also emerge as optimal neural codes for rapid signal transmission~\citep{Bethge03,Nikitin09}. This is also relevant for neuromorphic systems which transmit discrete events (such as a network packet). With a binary event-based code, the cost is only incremented when a new neuron gets active, regardless to the analog value. Stating that an active neuron carries a bounded amount of information of $\lambda$ bits, an upper bound for the representation cost of neural activity on the receiver end is proportional to the count of active neurons, that is, to the $\ell_0$ pseudo-norm $\| \coef \|_0$:%

% 
%\begin{equation}%
%\mathcal{C}_0( \coef | \image , \dico) = \frac{1}{2\sigma_n^2} \| \image - \dico \coef \|^2 + \lambda \| \coef \|_0 \nonumber%
%\end{equation}%

\begin{equation}%
\mathcal{C}_0( \coef | \image , \dico) = \frac{1}{2\sigma_n^2} \| \image - \dico \coef \|^2 + \lambda \| \coef \|_0%
\label{eq:L0_cost}%
\end{equation}%
This cost is similar with information criteria such as the Akaike Information Criteria~\citep{Akaike74} or distortion rate~\citep[p.~571]{Mallat98}. This simple non-parametric cost has the advantage of being dynamic: The number of active cells for one given signal grows in time with the number of spikes reaching the target population. But \seeEq{L0_cost} defines a harder cost to optimize (in comparison to Equation~\ref{eq:sparse_cost} for instance) since the hard $\ell_0$ pseudo-norm sparseness leads to a non-convex optimization problem which is \emph{NP-complete} with respect to the dimension $M$ of the dictionary~\citep[p.~418]{Mallat98}.

% the value of the positive coefficient does not matter much : more bits on address than on quantification: form of independenace...

 The sparse coding is obtained by minimizing a $\ell^0$ constrained optimization
%$$ \umin{ \norm{x}_0 \leq N_0 } \frac{1}{2}\norm{\image - \dico \coef}^2 .  $$
where the $\ell^0$ pseudo-norm of $\coef_i \in \RR^N$ is
\begin{equation} \norm{\coef_k}_0 = \abs{\enscond{i}{\coef_k(i) \neq 0}}. \end{equation}
The parameter $N_0>0$ controls the amount of sparsity that we impose to the coding.


It was found that by using an algorithm like Matching Pursuit, the learning algorithm could provide results similar to {\sc SparseNet}, but without the need of parametric assumptions on the prior~\citep{Perrinet10shl} by using this more generic $\ell_0$ norm sparseness. However, we observed that this class of algorithms could lead to solutions corresponding to a local minimum of the objective function: Some solutions seem as efficient as others for representing the signal but do not represent edge-like features homogeneously. In particular, during the early learning phase, some cells may learn ``faster'' than others. There is the need for a homeostasis mechanism that will ensure convergence of learning. The goal of this work is to study the specific role of homeostasis in learning sparse representations and to propose a homeostasis mechanism which optimises the learning of an efficient neural representation.%

Using the implemented sparse coding algorithms part of the sklearn library, we could control the convergence of the learning with different sparse coding algorithms. In particular we compared the learning as implemented with matching pursuit to that with  OMP, LARS or coordinate descent. 


\subsection{Learning to be sparse: Sparse Hebbian Learning}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
%We have seen above that we may define different models for measuring sparseness depending on our prior assumption on the distribution of coefficients.
%Note first that, assuming that the statistics are stationary (more generally ergodic), then these measures of sparseness across a population should necessary imply a lifetime sparseness for any neuron.
%Such a property is essential to extend results from electro-physiology.
%Indeed, it is easier to record a restricted number of cells
%than a full population (see for instance~\citep{Willmore11}).
%However, the main property in terms of efficiency  is that the representation should be sparse at any given time,
%that is, in our setting, at the presentation of each novel image.
%
%Now that we have defined sparseness, how could we use it to induce neural organization?
%Indeed, given a sparse coding strategy that optimizes any representation efficiency cost as defined above, we may derive an unsupervised learning model by optimizing the dictionary $\dico$ over natural scenes. On the one hand, the flexibility in the definition of the sparseness cost leads to a wide variety of proposed \emph{sparse coding} solutions (for a review, see~\citep{Pece02}) such as numerical optimization~\citep{Olshausen97}, non-negative matrix factorization~\citep{Lee99,Ranzato07} or Matching Pursuit~\citep{Perrinet03ieee,Smith06,Rehn07,Perrinet10shl}. They are all derived from correlation-based inhibition since this is necessary to remove redundancies from the linear representation. This is consistent with the observation that lateral interactions are necessary for the formation of elongated receptive fields~\citep{Bolz89,Wolfe10}.
%
%On the other hand, these methods share the same GLM model (see~\seeEq{lgm}) and once the sparse coding algorithm is chosen, the learning scheme is similar.
%As a consequence, after every coding sweep, we increased the efficiency of the dictionary $\dico$  with respect to \seeEq{efficiency_cost}. This is achieved using  the online gradient descent approach given the current sparse solution,  $\forall i$:
%\begin{equation}%
%\dico_{i} \la \dico_{i} + \eta \cdot a_{i} \cdot (\image - \dico\coef)%
%\label{eq:learn}%
%\end{equation}%
%where $\eta$ is the learning rate. %
%Similarly to Eq.~17 in~\citep{Olshausen97} or to Eq.~2 in~\citep{Smith06}, the relation is a linear ``Hebbian'' rule~\citep{Hebb49} since it enhances the weight of neurons proportionally to the correlation between pre- and post-synaptic neurons. Note that there is no learning for non-activated coefficients. The novelty of this formulation compared to other linear Hebbian learning rule such as~\citep{Oja82} is to take advantage of the sparse representation, hence the name Sparse Hebbian Learning (SHL).%
%
%The class of SHL algorithms are unstable without homeostasis, that is, without a process that maintains the system in a certain equilibrium. In fact, starting with a random dictionary, the first filters to learn are more likely to correspond to salient features~\citep{Perrinet03ieee} and are therefore more likely to be selected again in subsequent learning steps. In {\sc SparseNet}, the homeostatic gain control is implemented by adaptively tuning the norm of the filters. This method equalizes the variance of coefficients across neurons using a geometric stochastic learning rule. The underlying heuristic is that this introduces a bias in the choice of the active coefficients. In fact, if a neuron is not selected often, the geometric homeostasis will decrease the norm of the corresponding filter, and therefore ---from~\seeEq{lgm} and the conjugate gradient optimization--- this will increase the value of the associated scalar. Finally, since the prior functions defined in~\seeEq{sparse_cost} are identical for all neurons, this will increase the relative probability that the neuron is selected with a higher relative value. The parameters of this homeostatic rule have a great importance for the convergence of the global algorithm. In~\citep{Perrinet10shl}, we have derived a more general homeostasis mechanism derived from the optimization of the representation efficiency through histogram equalization which we will describe later (see Section~\ref{sec:laughlin}).
%%------------------------------%
%%: See Figure~\ref{fig:sparsenet}
%\begin{figure}%[ht!]%[p!]
%\centering{%
%\begin{tikzpicture}%[scale=1, font=\sffamily]%,every node/.style={minimum size=1cm},on grid]
%\draw [anchor=north west] (.33\textwidth, 0) node {\includegraphics[width=.33\textwidth]{sparsenet_map_ssc}};
%\draw [anchor=north west] (.66\textwidth, 0) node {\includegraphics[width=.33\textwidth]{sparsenet_efficiency-b}};
%\draw [anchor=north west] (.99\textwidth, 0) node {\includegraphics[width=.3275\textwidth]{sparsenet_efficiency-c}};
%\draw [anchor=north west] (.33\textwidth, .03\textwidth) node {$\mathsf{(A)}$};
%\draw [anchor=north west] (.66\textwidth, .03\textwidth) node {$\mathsf{(B)}$};
%\draw [anchor=north west] (.99\textwidth, .03\textwidth) node {$\mathsf{(C)}$};
%\end{tikzpicture}%
%}
%\caption{ {\bf Learning a sparse code using Sparse Hebbian Learning.}  \textsf{(A)}~We show the results  at convergence (20000 learning steps) of a sparse model with unsupervised learning algorithm which progressively optimizes the relative number of active (non-zero) coefficients ($\ell_0$ pseudo-norm)~\citep{Perrinet10shl}. Filters of the same size as the image patches are presented in a matrix (separated by a black border). Note that their position in the matrix is arbitrary as in ICA. These results show that sparseness induces the emergence of edge-like receptive fields similar to those observed in the primary visual area of primates. \textsf{(B)}~We show the probability distribution function of sparse coefficients obtained by our method compared to~\citep{Olshausen96} with first, random dictionaries (respectively 'ssc-init' and 'cg-init') and second, with the dictionaries obtained after convergence of respective learning schemes (respectively 'ssc' and 'cg'). At convergence, sparse coefficients are more sparsely distributed than initially, with more kurtotic probability distribution functions for {\sf 'ssc'} in both cases, as can be seen in the ``longer tails''of the distribution. \textsf{(C)}~We evaluate the coding efficiency of both methods by plotting the average residual error (L$_2$ norm) as a function of the $\ell_0$ pseudo-norm. This provides a measure of the coding efficiency for each dictionary over the set of image patches (error bars represent one standard deviation). Best results are those providing a lower error for a given sparsity (better compression) or a lower sparseness for the same error. %  (similarly to Occam's razor). %
%\label{fig:sparsenet}}%
%\end{figure}%
%%------------------------------%
dictionary learning aims at finding the best dictionary $\dico=(\dico_i)_{i=1}^N$ of $N$ atoms $\dico_i \in \RR^M$ such that they best represent the data. 
In particular, this could be a problem \emph{during} learning if we use the cost to measure representation efficiency for this learning step. An alternative is to 
perform a gradient descent on $\log[ F ] $:
$$ \frac{\partial }{\partial \dico_i } F = \frac{-1}{2 \sigma^2} \frac{\partial }{\partial \dico_i }[(\image - \dico \coef)^T (\image - \dico \coef)] = \frac{-1}{2 \sigma^2} \hat{\coef} (\image - \dico \hat{\coef})].$$
It is linear (at the single neuraon level) but the sparse code is not linear (at population level) - link with back-propagation


Dictionary learning performs an optimization both on the dictionary $D$
and the set of coefficients $ X = (x_j)_{j=1}^m \in \RR^{p \times m} $
where, for $j=1,\ldots,m$, $ x_j $
is the set of coefficients of the data $y_j$. This joint optimization reads
%$$ \umin{ D \in \Dd, X \in \Xx_k } E(X,D) = \frac{1}{2}\norm{Y-DX}^2 =
%\frac{1}{2} \sum_{j=1}^m \norm{y_j - D x_j}^2. $$
$$E(X,D) = \frac{1}{2}\norm{Y-DX}^2 =
\frac{1}{2} \sum_{j=1}^m \norm{y_j - D x_j}^2. $$


link of sparse coding with the cortical architecture (see rao and ballard) : 
- encoder / decoder
- variational auto-encoder
- sensory error versus prediction error


Ringach02

\subsection{Algorithm: Optimal homeostasis}\label{HEH}
% - a measure is that the Orientation Selectivity (OS) is highest - because we know that the structure of the world is like this...
% - goals = memory / separation / representation

Knowing a dictionary and a sparse coding algorithm, we may transform any data $\image$ into a set of sparse coefficients. As we have seen, 

% we know that during learning, there are distortions in the code / dictionary : normalisation is not enough.
During the early learning phase, some cells may learn ``faster'' than others. There is the need for a homeostasis mechanism that will ensure convergence of learning. The goal of this work is to study the specific role of homeostasis in learning sparse representations and to propose a homeostasis mechanism which optimises the learning of an efficient neural representation. 
% method zero = just normalizing the coefficitents (Mairal) -  first method = Olshausen's homeostasis that is a gradient descent on the variance of coefficients. serves as a control


To achieve this, we first formulate analytically the problem of representation efficiency in a population of sensory neurones. For the particular $\ell_0$ norm sparseness, we show that sparseness is optimal, in term of Shannon entropy, when average activity within the neural population is uniformly balanced (i.e. each neurone is selected with the same probability when encoding a large set of data). To achieve this uniformity, we define an homeostatic gain control mechanism based on histogram equalisation, that is in transforming coefficients in terms of z-scores $z_i(a_i) = P( \cdot > a_i)$. The cumulative distribution $z_i$ for each coefficient of the sparse vector is calculated using Hebbian learning to smooth its evolution during learning. At the coding level, this z-score function is incorporated in the matching step of the matching pursuit algorithm, to modulate the choice of the most  as that with the maximal z-score: $i^\ast = \mathrm{Argmax}_i z_i(a_i)$. The rest of the algorithm is left unchanged.

% using cumulative distribution = "inverse transform sampling" / a monotonic point scalar function does not change the LK distance / free-energy
In particular, we will set the a priori probability of selecting coefficients {\color{BrickRed} $\forall (i,j), P(\coef_i)=P(\coef_j)$} to ensure the optimality of the choice of the  pseudo $\ell_0$-norm and compare it to the representation in the primary visual cortex.%


\subsection{Computational implementation}
Finally, the unsupervised learning can be summarized using the following steps:
The proposed algorithm is:
\begin{enumerate}%
{\color{MidnightBlue} 
\item Initialize the point non-linear gain functions $z_i$ to similar cumulative distribution functions and the components $\dico_i$ to random points on the unit $L$-dimensional sphere,%
\item repeat until learning converged:%
\begin{enumerate}%
{\color{OliveGreen}
	\item draw a signal $\image$ from the database, its energy is $E = \| \image \|^2$,%
	\item set sparse vector $\coef$ to zero, initialize $\bar{a}_i=<\image, \dico_i >$ for all $i$,% using~\seeEq{coco},%
	\item while the residual energy $E$ is above a given threshold do:
	\begin{enumerate}
		{\color{BrickRed}
			\item select the best match: $i^\ast = \mbox{ArgMax}_{i} [z_i( \bar{a}_i )]$,% with~\seeEq{mp1},
		}
		\item set the sparse coefficient: $a_{i^\ast} = \bar{a}_{i^\ast}$,
		\item update residual coefficients: $\forall i, \bar{a}_i \leftarrow \bar{a}_i - a_{i^\ast} <\dico_{i^\ast} , \dico_i > $,% for all $i$ using~\seeEq{mp3},
		\item update energy: $E \leftarrow E - a_{i^\ast}^2 $.
	\end{enumerate}
}
\item when we have the sparse representation vector $\coef$, apply $\forall i$:
\begin{enumerate}
\item modify dictionary: $\dico_{i} \leftarrow \dico_{i} + \eta a_{i} (\image - \dico\coef)$,% using~\seeEq{learn}, 
\item normalize dictionary: $\dico_{i} \leftarrow \dico_{i} / \| \dico_{i}\|$,% using~\seeEq{learn}, 
{\color{BrickRed}
\item update homeostasis functions: $z_i( \cdot ) \leftarrow (1- \eta_h ) z_i( \cdot ) + \eta_h \delta( a_i \leq \cdot)$.% using~\seeEq{learn_homeo}
}
\end{enumerate}
\end{enumerate}
}
\end{enumerate}

\subsection{Quantitative Evaluation of dictionaries}
%%------------------------------%
%\begin{figure}[!hb]%[!ht]%%[p!]
%\centering{
%\begin{tikzpicture}
%\draw [anchor=north west] (.05\linewidth, .435\linewidth) node {\includegraphics[height=.435\linewidth]{figures/dictionary_example.pdf}};
%\draw [anchor=north west] (.5\linewidth, .45\linewidth) node {\includegraphics[width=.4\linewidth]{figures/angular_tuning.pdf}};
%\draw [anchor=north west] (.5\linewidth, .2\linewidth) node {\includegraphics[width=.4\linewidth]{figures/frequency_tuning.pdf}};
%\draw (0, .4\linewidth) node [above right=0mm] {$\mathsf{A}$};
%\draw (.45\linewidth, .4\linewidth) node [above right=0mm] {$\mathsf{B}$};
%\draw (.45\linewidth, .2\linewidth) node [above right=0mm] {$\mathsf{C}$};
%\end{tikzpicture}}
%\caption{
%{\bf  Receptive fields for increasing number of population cells}: 
%%
%We show in ($\mathsf{A}$) some cells drawn at random from populations of increasing sizes $M$ after the convergence ($20000$ learning steps) of an unsupervised learning algorithm. The algorithm was trained on $32\times 32$ patches drawn from natural images which were selected and pre-processed in a similar way as in the {\sc SparseNet} algorithm~\citep{olshausen1996emergence}. 
%%A first qualitative result is that learning converged for all different population sizes  to localized features within the patches. 
%Qualitatively, cells' selectivity resembled that present in the primary visual cortex but differed from the population with the lowest complexity to the  bigger populations.
%%
%%\label{fig:map}
%{\bf Quantitative analysis of the different populations of cells}: Using the response of cells in the populations to gratings of different orientations and spatial frequency, we show the ($\mathsf{B}$) orientation and ($\mathsf{C}$) frequency selectivity as measured by the standard deviation of tuning curves. While orientation tuning saturates around $M=400$, frequency tuning gets finer for larger populations ($M > 800$). %  of increasing sizes
%}%
%\end{figure}%
%%%------------------------------%

We compared qualitatively the set $\dico$ of receptive filters generated by the proposed algorithm when the homeostasis is first turned-off and then enabled  (see Fig.~\ref{fig:map}). A more quantitative study of the coding is shown by comparing selection distribution of sparse coefficients when the homeostasis mechanism is turned on (see Fig.~\ref{fig:quant}). We demonstrate that forcing the learning activity to be uniformly spread among all receptive fields results in a faster convergence of the representation error, and in an increase of the Shanon entropy. 
%------------------------------%
%: see Figure~\ref{fig:HEH}
\begin{figure}[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .8\linewidth) node {\includegraphics[width=\linewidth]{PDF_nohomeo}};
%\draw [anchor=north west] (0, .4\linewidth) node {\includegraphics[width=\linewidth]{z_score}};
%\draw [anchor=north west] (0, .0) node {\includegraphics[width=\linewidth]{PDF_homeo}};
\draw (.7\linewidth, 0) node [above right=0mm] {$\mathsf{A}$};
\draw (.3\linewidth, 0) node [above right=0mm] {$\mathsf{B}$};
\draw (-.1\linewidth, 0) node [above right=0mm] {$\mathsf{C}$};
\draw (.0\linewidth, .38\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.85\linewidth, .38\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Quantitative role of homeostasis in sparse coding}: We show the results of Sparse Coding using the two different homeostasis algorithms using surrogate data where each filter was equiprobable but for which we manipulated the first half of the coefficients to be artificially twice as big. %
{\sf (A)}~Such a situation replicates a situation arising during learning when a sub-group of filters is more active, e.~g. because it learned more salient features.  Here, we show the probability of the selection of the different filters (normalized to an average of $1$) which shows a bias of the standard Matching Pursuit to select more often filters whose activity is higher. %We evaluated the efficiency of retrieving the correct coefficients to about $\ %
{\sf (B)}~Non-linear homeostatic functions learned using Hebbian learning. These functions were initialized as the cumulative distribution function of uniform random variables. Then they are used to modify choices in the Matching step of the Matching Pursuit algorithm. Progressively, the non-linear functions converge to the (hidden) cumulative distributions of the coefficients of the surrogate, clearly showing the group of filters with twice a big coefficients. 
 {\sf (C)}~At convergence, the probability of choosing any filter is uniform. As a result, entropy is maximal, a property which is essential for the optimal representation of signals in distributed networks such as the brain.
\label{fig:HEH}}%
\end{figure}%
%%------------------------------%


% LUP\ IS\ HERE\ 



\section{Results: Fast unsupervised learning using homeostasis}\label{results}
%------------------------------%
%: see Figure~\ref{fig:HAP}
\begin{figure}[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .39\linewidth) node {\includegraphics[width=.47\linewidth]{dico_MP}};
%\draw [anchor=north west] (.5\linewidth, .39\linewidth) node {\includegraphics[width=.47\linewidth]{dico_SN}};
%\draw [anchor=north west] (.25\linewidth, -.1\linewidth) node {\includegraphics[width=.47\linewidth]{dico_MEUL}};
%\draw [anchor=north west] (.1\linewidth, -.6\linewidth) node {\includegraphics[width=.8\linewidth]{Comparison_reconstruction}};
\draw (0, .39\linewidth) node [above right=0mm] {$\mathsf{A}$};
\draw (.5\linewidth, .39\linewidth) node [above right=0mm] {$\mathsf{B}$};
%\draw (.2\linewidth, -.16\linewidth) node [above right=0mm] {$\mathsf{C}$};
%\draw (.05\linewidth, -.67\linewidth) node [above right=0mm] {$\mathsf{D}$};
\draw (.0\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.85\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Quantitative evaluation of fast unsupervised learning}: We used the HAP approximation. %
 {\sf (A)}~30 from the 324 dictionaries learned {\sf (B)}~Comparison of the reconstruction error (computed as the square root of the squared difference between the image and the residual) for the 3 algorithms (HEH, SPARSENET, HAP): The convergence velocity of MEUL is higher than SPARSENET and MP.
\label{fig:HAP}}%
%\caption{
%{\bf Quantitative role of homeostasis in a classification network}: We used the generic MNIST protocol to assess the role of the homeostasis algorithm on classification. %
% {\sf (A-C)}~144 dictionaries learned from the MNIST database with a sparseness of 5 after 10000 iterations with {\sf (A)}~MP Algorithm ($\eta=0.01$): No homeostasis regulation, only a small subset of dictionaries are selected with a high probability to describe the dataset.
%{\sf (B)}~SPARSENET Algorithm ($\eta=0.01$, $\eta_h=0.01$, $\alpha_h=0.02$): The homeostasis regulation is made by normalizing the volatility.
%{\sf (C)}~MEUL Algorithm ($\eta=0.01$, $\eta_h=0.01$): All dictionaries are selected with the same probability to describe the dataset, leading to a cooperative learning.
% {\sf (D)}~Comparison of the reconstruction error (computed as the square root of the squared difference between the image and the residual) for the 3 algorithms (MEUL, SPARSENET, MP): The convergence velocity of MEUL is higher than SPARSENET and MP.
%\label{fig:quant}}%
\end{figure}%
%%------------------------------%

\subsection{Algorithm: Approximate homeostasis}\label{HAP}
%: incompatible with nueromimetic / fast implementation
We have shown above that we can find an exact solution to the problem of homeostasis during sparse hebbian learning. However, this solution has several drawbacks. First, it is computationally intensive on a conventional computer as it necessitates to store the cumulative distribution of each coefficient. More importantly, it seems that biological neurons seem to rather use a simple gain control mechanism. This is similar to that used by Olshausen. This is similar to the mechanisms of gain normalization proposed by Schwartz and which were recently shown to provide efficient coding mechanisms by Simoncelli. This simplifies our algorithm above by removing step d.iii and using for each atom as a non-linear transform the rectified linear unit (ReLU) with gain $\gamma_i>0$:  $z_i (\coef_i) = \gamma_i * \coef_i * \delta(\coef_i>0)$. However, compared to these methods which manipulate the gain of dictionaries based on the energy of coefficients, we propose to rather use a methodology based on the probability of activation. 

%: Comparison to Sandin
Recently, a similar approach was proposed by Sandin. Based on the same observations, authors propose to optimise the coding during learning by modulating the gain of each dictionary element based on the recent history. They base their Equalitarian Matching Pursuit  algorithm on the following heuristics which cancels the activation of all filters that were more often activated than a given threshold:
$$ \gamma_i = \delta (p_i <  N_0/N*(1+\alpha_h)  $$
were $\delta$ is Kronecker's indicator function and  $\alpha_h$ is the threshold coefficient. 

 / correlation but now on the activation probability

%: derivation
Similarly, we may derive an approximate homeostasis algorithm based on the current activation probability but using a gradient descent approach on gain modulation. 
For this we need to derivate the change of modulation gain that would be necessary to achieve an equiprobable 

From this we derive the following rule:

We will coin this algorithm variant as Homeostasis on Activation Probability (HAP).

% derivation p.56N de 2018-01)09 HULK

\subsection{Approximate homeostasis yields similar performance}
Following these derivations we quantitatively compared HEH, EMP and HAP. This shows 

% effect of coding algorithm
In particular, we replicated the result of Sandin that while homeostasis was essential in improving unsupervised learning, the coding algorithm (MP versus OMP) mattered relatively little. 

%: effect of eta, eta_homeo

%: structural complexity: effect of size fo dictionary, 

\section{Discussion and conclusion}\label{discussion-et-conclusion}

% Plausibility of such a mechanism (see Laughlin et al)

%: 2- benchmarking of computation time: toward event-driven?

% Finally, we show that such an algorithm can be extended to convolutional architectures and we show the results obtained on different natural image databases. 
%
One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any sample as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that sparse coding can be optimized by designing a proper homeostatic rule regulating the competition between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalisation similar to what is observed in biological neural networks. We validated this theoretical insight by challenging the matching pursuit sparse coding algorithm with the same learning rule but with or without homeostasis. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding accuracy did not vary much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we will apply this algorithm to the optimisation of a visual system to be embedded in an aerial robot. In summary, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms. 
 %The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. 

%2. importance of homeo - vanishing term in deep learning -> use deep learning to validate output
%3. application to asynchronous / focal  log-polar (retinal) input / continuous learning / credit assignement (no access to true residual)

To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. Indeed, such robots have an increasing demand for visual applications, such as computation of optic flow for flight stabilization or for common visual categorization tasks such as recognition of pilot's gestures. However, embedding such a system is often difficult as it necessitates to be rapid,  energy efficient and lightweight. We resolved this problem by introducing our novel algorithm as an output of our visual sensors (the ``retina''). Results were collected by comparing the image reconstruction error for different information bandwidths in the output bus of this retina.Quantitative results show that the quality of the reconstruction was improved in our algorithm on a  database of natural, flying videos of the aerial robot.In particular, this algorithm provides with a real-life example showing how to improve our understanding of the emergence of edge-selective simple cells, drawing the bridge between structure (representation) and function (efficient coding).% 
%


 By developing this
fast learning algorithm we hope for its rapid application in artificial
intelligence algorithms. This type of architecture is economical,
efficient and fast. It makes it possible to be transferred to most deep
learning algorithms, a major flaw of which is to be very greedy in
computing resources. In particular, we are considering perspectives for
coding within a dynamic flow of sensory data and we hope to apply this
type of algorithm on embedded systems such as aerial robots. Along with
this, we hope that this new type of rapid unsupervised learning
algorithm can provide a normative theory for the coding of information
in low-level sensory processing, whether it is visual or auditory, for
example.

Moreover, by its nature, this algorithm can easily be extended to convolutional networks such as those used in deep learning neural networks. This extension is possible by extending the filter dictionary by the hypothesis of invariances to the translation of representations. Our results on different databases show the stable and rapid emergence of characteristic filters on these different bases. This result shows a probable prospect of extending this representation and for which we hope to obtain classification results superior to the algorithms existing in the state-of-the-art. Finally we will conclude by showing the importance of homeostasis in unsupervised learning algorithms.


%%%%%----------------------------------------------------------------- 
%%\section*{Acknowledgments}
%%%{\bf Acknowledgments}\\
%%\Acknowledgments

%%%----------------------------------------------------------------- 
\printbibliography

\end{document}