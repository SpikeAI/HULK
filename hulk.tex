%!TEX root = hulk.tex
%!TeX TS-program = Lualatex
%!TeX encoding = UTF-8 Unicode
%!TeX spellcheck = en-US
%!BIB TS-program = bibtex
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Draft{0}%
\def\Draft{1}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\if 1\Draft
\documentclass[a4paper, 11pt, draft]{article} % For LaTeX2e
\else
\documentclass[a4paper, 11pt]{article} % For LaTeX2e
\fi
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2018}
%\usepackage[final]{nips_2018}
%
%============ common ===================
\usepackage[utf8]{luainputenc}
\usepackage[english]{babel}%
%\usepackage{csquotes}%
\usepackage[autostyle]{csquotes}
%% Sans-serif Arial-like fonts
\renewcommand{\rmdefault}{phv} 
\renewcommand{\sfdefault}{phv} 
     \usepackage{textcomp}
     \usepackage{libertine}%[sb]
     \usepackage[varqu,varl]{inconsolata}% sans serif typewriter
     \usepackage[libertine,bigdelims,vvarbb]{newtxmath} % bb from STIX
     \usepackage[cal=boondoxo]{mathalfa} % mathcal
%     \useosf % osf for text, not math
     \usepackage[supstfm=libertinesups,%
       supscaled=1.2,%
       raised=-.13em]{superiors}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
% --------------------------------------------------------------------------
%                    METADATA
% --------------------------------------------------------------------------
\newcommand{\AuthorA}{Boutin}%
\newcommand{\FirstNameA}{Victor}%
\newcommand{\AuthorB}{Franciosini}%
\newcommand{\FirstNameB}{Angelo}%
\newcommand{\AuthorC}{Perrinet}%
\newcommand{\FirstNameC}{Laurent U}%
\newcommand{\Institute}{Institut Neurosciences Timone, Marseille, France}%\\ Aix Marseille Univ, CNRS}%
\newcommand{\Organism}{Aix Marseille Universit\'e, CNRS}%
\newcommand{\Address}{27, Bd. Jean Moulin, 13385 Marseille Cedex 5, France}%
\newcommand{\Website}{http://invibe.net/LaurentPerrinet}%
\newcommand{\EmailA}{victor.boutin@univ-amu.fr}%
\newcommand{\EmailB}{angelo.franciosini@univ-amu.fr}%
\newcommand{\EmailC}{laurent.perrinet@univ-amu.fr}%victor.boutin@univ-amu.fr
\newcommand{\WebsiteC}{http://invibe.net/LaurentPerrinet}%
%Submission Format
%
%Before you log onto the submission website, you should have the following items prepared. Submissions that do not meet these guidelines may be rejected. Abstract selection is a competitive process, so please read carefully:
%
%    * Title - 100 characters or fewer (including spaces), capitalized in sentence case
%    * Author list - including email addresses of all authors
%    * Summary - 300 words or fewer, text only. This summary will appear in the printed conference program if your submission is accepted.
%    * PDF submission - This should be one page (A4 or US Letter) in PDF format. The title and author names should appear at the head of the page (contact info may be omitted), followed by the 300-word Summary. The rest of the document should expand upon the central question(s), approach, results, and/or conclusions of the study. Figures are optional. You may include equations as appropriate. Font size (including any figure legends) must be at least 12 point. Margins should be at least 0.5". Because space is limited, you do not need to touch upon all the major points of the Summary; rather you should aim to add whatever detail is need to help reviewers evaluate the technical correctness and significance of your study.
%    * Two Program Committee members (see below) who could handle your submission. We cannot honor all specific requests, but your list will help us direct your submission to Program Committee members with appropriate expertise. 
%
%Evaluation Criteria
%
%PDF submissions will be evaluated on the basis of the following criteria (equally weighted):
%
%    * Significance, originality, and potential impact of your claims to the COSYNE audience. Please indicate why your question is important and how your claims advance the field. Reviewers will be asked to rate your claims in this regard, without regard to whether your methods/results technically support those claims (see below). If your aim is to develop a new approach or technique, indicate why this is useful. Implicitly included in this category is the quality of fit of your abstract for the COSYNE conference audience -- potentially inappropriate abstracts include pure machine learning studies, or studies of single cells with no clear implications for neural systems.
%    * Technical correctness. Reviewers will be asked to rate the quality of your methods and/or data in supporting your claims (see above). Thus, you should clearly outline the methods used, results obtained, and critical controls. Include sufficient detail in your submission to allow reviewers to evaluate whether your approach is appropriate for supporting your claims, and whether your results are technically sound. However, your submission should not be as detailed as the Methods section of a full-length paper (e.g., do not list concentrations of drugs, etc.). 
%
%You should not feel obliged to fill the entire page, and we expect that many successful submissions will not include figures.
%
%Approximately 20 submissions will be chosen for short talks and ~300 will be chosen for poster presentations. We expect ~20% of submissions will be rejected due to limitations on poster space. 
\newcommand{\Title}{%
A fast algorithm for unsupervised learning
%alternatives: - Homeostasis is necessary for the unsupervised learning of orientation-selective cells
%# Homeostatic Unsupervised Learning of Kernels
}%
\newcommand{\Abstract}{
The formation of structure in the brain, that is, of the connection between cells within neural populations, is by large an unsupervised learning process: The emergence of this architecture is mostly self-organized. In the primary visual cortex of mammals, for example, one may observe during development the emergence of cells selective to localized, oriented features. This leads to the development of a rough representation of contours of the retinal image in area V1. A major difficulty in defining unsupervised learning algorithms is that during this process, on the one hand the coding is performed knowing an immature structure and on the other hand, the adaptation of this structure is carried out knowing a code that is not yet optimal. We propose here a fast algorithm compatible with a neuromimetic architecture which solves this problem and allows for the fast emergence of localized filters sensitive to orientation. The key to this algorithm lies in a simple yet optimal mechanism of homeostasis that reconciles the antagonistic processes that occur at the coding and learning time scales. We tested this unsupervised algorithm with this homeostasis rule for a range of existing unsupervised learning algorithms coupled with different neural coding algorithms. In addition, we propose a simplification of this optimal homeostasis rule by implementing a simple heuristic on the probability of activation of neurones. Compared to the optimal homeostasis rule, we show that this heuristic allows to implement an even faster unsupervised learning algorithm while keeping a large part of its effectiveness. These results demonstrate the potential application of such a strategy to the fast classification of images, for example in hierarchical and dynamic architectures.
}
\newcommand{\Keywords}{Vision, Sparseness, Computer vision, Unsupervised learning, Neuroscience}%
\newcommand{\Acknowledgments}{%
This work was supported by XXX and the Doc2Amu project which received funding from a co-fund with the European Union's Horizon 2020 research and innovation programme and the region Provence Alpes Cote d'Azur. }
\newcommand{\Links}{%
\begin{itemize}
\item Correspondence and requests for materials should be addressed to LUP (email:\EmailC ). 
\item Code and supplementary material available at \url{\WebsiteC/Publications/BoutinFranciosiniPerrinet18hulk}.
\end{itemize}
} %
% --------------------------------------------------------------------------
\usepackage{filecontents}

\begin{filecontents}{hulk.bib}

@article{olshausen1996emergence,
  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author={Olshausen, Bruno A and Field, David J},
  journal={Nature},
  volume={381},
  number={6583},
  pages={607},
  year={1996},
  publisher={Nature Publishing Group}
}

@article{Olshausen97,
	Abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable to the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images (Olshausen and Field, 1996a). Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete---i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output fun...},
	Author = {Olshausen, Bruno A. and Field, David J.},
	Citeulike-Article-Id = {9026860},
	Citeulike-Linkout-0 = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
	Citeulike-Linkout-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.712},
	Date-Added = {2017-01-02 14:34:49 +0000},
	Date-Modified = {2017-01-02 14:34:49 +0000},
	Doi = {10.1016/S0042-6989(97)00169-7},
	Issn = {0042-6989},
	Journal = {Vision Research},
	Keywords = {area-v1, assofield, bicv-sparse, sparse\_hebbian\_learning},
	Month = dec,
	Number = {23},
	Pages = {3311--3325},
	Priority = {0},
	Title = {Sparse coding with an overcomplete basis set: A strategy employed by {V1}?},
	Url = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
	Volume = {37},
	Year = {1997},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7}}


@article{hubel1968receptive,
  title={Receptive fields and functional architecture of monkey striate cortex},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of {P}hysiology},
  volume={195},
  number={1},
  pages={215--243},
  year={1968},
  publisher={Wiley Online Library}
}


@article{Serre07,
abstract = {Primates are remarkably good at recognizing objects. The level of performance of their visual system and its robustness to image degradations still surpasses the best computer vision systems despite decades of engineering effort. In particular, the high accuracy of primates in ultra rapid object categorization and rapid serial visual presentation tasks is remarkable. Given the number of processing stages involved and typical neural latencies, such rapid visual processing is likely to be mostly feedforward. Here we show that a specific implementation of a class of feedforward theories of object recognition (that extend the Hubel and Wiesel simple-to-complex cell hierarchy and account for many anatomical and physiological constraints) can predict the level and the pattern of performance achieved by humans on a rapid masked animal vs. non-animal categorization task.},
author = {Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
doi = {10.1073/pnas.0700622104},
file = {:Users/lolo/quantic/science/bibtexing/Serre, Oliva, Poggio/Serre, Oliva, Poggio_2007_A feedforward architecture accounts for rapid categorization.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences},
keywords = {assofield,bicv-sparse,perrinet11sfn,perrinetbednar15},
mendeley-tags = {assofield,perrinet11sfn},
number = {15},
pages = {6424--6429},
pmid = {17404214},
publisher = {National Academy of Sciences},
title = {{A feedforward architecture accounts for rapid categorization}},
type = {Journal article},
url = {http://dx.doi.org/10.1073/pnas.0700622104 http://www.pnas.org/content/104/15/6424.abstract http://www.pnas.org/content/104/15/6424.full.pdf http://www.pnas.org/cgi/content/abstract/104/15/6424},
volume = {104},
year = {2007}
}
@article{Carandini12,
abstract = {There is increasing evidence that the brain relies on a set of canonical neural computations, repeating them across brain regions and modalities to apply similar operations to different problems. A promising candidate for such a computation is normalization, in which the responses of neurons are divided by a common factor that typically includes the summed activity of a pool of neurons. Normalization was developed to explain responses in the primary visual cortex and is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions. Normalization may underlie operations such as the representation of odours, the modulatory effects of visual attention, the encoding of value and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that it serves as a canonical neural computation.},
author = {Carandini, Matteo and Heeger, David J Dj},
doi = {10.1038/nrn3136},
isbn = {1471-0048 (Electronic)\n1471-003X (Linking)},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Adaptation,Afferent Pathways,Anim,Physiological,contrast_response,divisive_normalization,normalization},
mendeley-tags = {contrast_response,divisive_normalization,normalization},
month = {jan},
number = {November},
pages = {1--12},
pmid = {22108672},
publisher = {Nature Publishing Group},
title = {{Normalization as a canonical neural computation.}},
type = {Journal article},
url = {http://discovery.ucl.ac.uk/1332718/ http://www.ncbi.nlm.nih.gov/pubmed/22108672 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3273486},
volume = {13},
year = {2012}
}

@article{PerrinetBednar15,
	Abstract = {Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field'' for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category.},
	Author = {Perrinet, Laurent U. and Bednar, James A.},
	Doi = {10.1038/srep11400},
	Issn = {2045-2322},
	Journal = {Scientific Reports},
	Keywords = {anr-trax,assofield,bicv-sparse,perrinetbednar15,sanz12jnp,vacher14},
	Language = {en},
	Month = {jun},
	Pages = {11400},
	Pmid = {26096913},
	Publisher = {Nature Publishing Group},
	Title = {{Edge co-occurrences can account for rapid categorization of natural versus animal images}},
	Url = {http://www.nature.com/doifinder/10.1038/srep11400 http://www.nature.com/srep/2015/150622/srep11400/full/srep11400.html http://www.nature.com/articles/srep11400},
	Volume = {5},
	Year = {2015},
	Bdsk-Url-1 = {http://www.nature.com/doifinder/10.1038/srep11400%20http://www.nature.com/srep/2015/150622/srep11400/full/srep11400.html%20http://www.nature.com/articles/srep11400},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/srep11400}}


@article{Perrinet10shl,
	Abstract = {Neurons in the input layer of primary visual cortex in primates develop edge-like receptive fields. One approach to understanding the emergence of this response is to state that neural activity has to efficiently represent sensory data with respect to the statistics of natural scenes. Furthermore, it is believed that such an efficient coding is achieved using a competition across neurons so as to generate a sparse representation, that is, where a relatively small number of neurons are simultaneously active. Indeed, different models of sparse coding coupled with Hebbian learning and homeostasis have been proposed that successfully match the observed emergent response. However, the specific role of homeostasis in learning such sparse representations is still largely unknown. By quantitatively assessing the efficiency of the neural representation during learning, we derive a cooperative homeostasis mechanism which optimally tunes the competition between neurons within the sparse coding algorithm. We apply this homeostasis while learning small patches taken from natural images and compare its efficiency with state-of-the-art algorithms. Results show that while different sparse coding algorithms give similar coding results, the homeostasis provides an optimal balance for the representation of natural images within the population of neurons. Competition in sparse coding is optimized when it is fair: By contributing to optimize statistical competition across neurons, homeostasis is crucial in providing a more efficient solution to the emergence of independent components.},
	Annote = {Posted Online March 17, 2010.},
	Author = {Perrinet, Laurent U.},
	Citeulike-Article-Id = {7158387},
	Citeulike-Linkout-0 = {http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl},
	Citeulike-Linkout-1 = {http://dx.doi.org/10.1162/neco.2010.05-08-795},
	Citeulike-Linkout-2 = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.2010.05-08-795},
	Citeulike-Linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/20235818},
	Citeulike-Linkout-4 = {http://www.hubmed.org/display.cgi?uids=20235818},
	Date-Added = {2017-01-02 13:44:42 +0000},
	Date-Modified = {2017-01-02 13:44:42 +0000},
	Day = {17},
	Doi = {10.1162/neco.2010.05-08-795},
	Issn = {1530-888X},
	Journal = {Neural Computation},
	Keywords = {adaptive, assofield, bicv-sparse, cell, coding, competition-optimized, cooperative, fields, hebbian, homeostasis, images, khoei13jpp, learning, matching, matching-pursuit, natural, natural-scenes, neural, of, overcomplete\_dictionaries, perrinet10shl, perrinet11sfn, perrinet12pred, population, pursuit, receptive, sanz12jnp, simple, sparse, sparse\_coding, sparse\_hebbian\_learning, sparse\_spike\_coding, statistics, unsupervised, vacher14},
	Month = jul,
	Number = {7},
	Pages = {1812--36},
	Pmid = {20235818},
	Priority = {0},
	Publisher = {MIT Press},
	Title = {Role of homeostasis in learning sparse representations},
	Url = {http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl},
	Volume = {22},
	Year = {2010},
	Bdsk-Url-1 = {http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl},
	Bdsk-Url-2 = {http://dx.doi.org/10.1162/neco.2010.05-08-795}}

@incollection{Perrinet15bicv,
	Author = {Perrinet, Laurent U.},
	Booktitle = {Biologically Inspired Computer Vision},
	Chapter = {13},
	Citeulike-Article-Id = {13566753},
	Date-Added = {2017-01-02 13:44:42 +0000},
	Date-Modified = {2017-01-02 13:44:42 +0000},
	Editor = {Crist{\'{o}}bal, Gabriel and Perrinet, Laurent and Keil, Matthias S.},
	Isbn = {9783527680863},
	Keywords = {bicv-sparse},
	Month = {nov},
	Priority = {2},
	Publisher = {Wiley-VCH Verlag},
	Title = {Sparse Models for Computer Vision},
	Url = {http://onlinelibrary.wiley.com/doi/10.1002/9783527680863.ch14/summary},
	Year = {2015},
	Bdsk-Url-1 = {http://onlinelibrary.wiley.com/doi/10.1002/9783527680863.ch14/summary}}
\end{filecontents}
\usepackage[natbib=true,
			style=ieee, %numeric-comp,
      		sortcites=true,
			abbreviate=true,
			maxcitenames=2,
			maxnames = 5,
%			firstinits=true,
%			uniquename=init,
%			sorting=none,
			doi=false,
			url=true,
			isbn=false,
			eprint=false,
			texencoding=utf8,
			bibencoding=latin1,
			%autocite=superscript,
			backend=bibtex,
			%articletitle=false,
			]{biblatex}%
\AtEveryBibitem{
  \clearfield{month}
  \clearfield{day}
  \clearfield{url}
  \clearfield{note}
  \clearfield{comment}
%  \clearfield{edition}
%  \clearfield{publisher}
}
\addbibresource{hulk.bib}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[unicode,linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue,pdfborder={0 0 0}]{hyperref}%
%\hypersetup{%
%pdftitle={\Title},%
%pdfauthor={Corrresponding author: \AuthorC < \EmailC > \Address - \WebsiteC },%
%pdfkeywords={\Keywords},%
%pdfsubject={\Acknowledgments}%
%}%
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[dvipsnames]{xcolor}

\usepackage{tikz}%
%\if 1\Draft
%\usepackage{setspace}
%\fi
% MATHS (AMS)
%\usepackage{amsmath}
%%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{amsfonts, amssymb, amscd}
%: symbols
\newcommand{\umin}[1]{\underset{#1}{\min}\;}
\newcommand{\enscond}[2]{\lbrace #1, #2 \rbrace}
\newcommand{\norm}[1]{|\!| #1 |\!|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
\newcommand{\eqdef}{\ensuremath{\stackrel{\mbox{\upshape\tiny def.}}{=}}}
\newcommand{\eqset}{\ensuremath{\stackrel{\mbox{\upshape\tiny set}}{=}}}
\newcommand{\eq}[1]{\begin{equation*}#1\end{equation*}}
\newcommand{\eql}[1]{\begin{equation}#1\end{equation}}
\newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
\newcommand{\NN}{\mathbb{N}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\CC}{\mathbb{C}}
\usepackage{siunitx}
%\renewcommand{\cite}{\citep}%
\newcommand{\ms}{\si{\milli\second}}%
\newcommand{\m}{\si{\meter}}%
\newcommand{\s}{\si{\second}}%


\newcommand{\coef}{\mathbf{a}} % image's hidden param
\newcommand{\image}{\mathbf{I}} % the image
\newcommand{\dico}{\Phi} % the dictionary
%\showthe\columnwidth
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
%\usepackage{titling}
%\setlength{\droptitle}{-6em}
%\posttitle{\par\end{center}\vspace{-2.3em}}

\title{\Title}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{%
%\makebox[.45\linewidth]{\FirstNameA\ \AuthorA}
%\and \makebox[.45\linewidth]{\FirstNameB\ \AuthorB}% \\ \Institute\ \Organism
%\makebox[.9\linewidth]{\FirstNameA\ \AuthorA and \FirstNameB\ \AuthorB} %\\ \Institute\
\FirstNameA\ \AuthorA \and \FirstNameB\ \AuthorB \and \FirstNameC\ \AuthorC
}
\date{\Institute\ \\ 
\Organism\
}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

%\author{
%David S.~Hippocampus\thanks{Use footnote for providing further
%information about author (webpage, alternative
%address)---\emph{not} for acknowledging funding agencies.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213 \\
%\texttt{hippo@cs.cranberry-lemon.edu} \\
%% examples of more authors
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \AND
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%}

%\setlength{\pdfpagewidth}{\paperwidth}
%\setlength{\pdfpageheight}{\paperheight}
\usepackage{times}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

%\usepackage{fancyhdr}


%\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
%\usepackage[margin=0.5in]{geometry}
%\oddsidemargin 0.0in		% margin, in addition to 1" standard
%\textwidth 6.5in		% 8.5" - 2*(1+\oddsidemargin)
%
%%\topmargin -0.25in		% in addition to 1.5" standard margin
%\topmargin -.75in		% in addition to 1.5" standard margin
%\textheight 8.75in 		% 11 - ( 1.5 + \topmargin + <bottom-margin> )
%
%\columnsep 0.1in
%
%\parindent 0pt
%\parskip 12pt
%
%\flushbottom \sloppy
%\topskip    = 0.0truein
%\topmargin  = 0.0truein
%\headheight = 0.0truein
%\headsep    = 0.0truein
%\hoffset    = -0.48truein
%\voffset    = -0.9truein
%\parskip    = \baselineskip
%\parindent  = 0pt
%\textwidth  = 6.4truein
%\textheight = 10.1truein
%\flushbottom\makeatletter

%\pagestyle{empty}		% No page numbers
\begin{document}
%
% make the title area
\maketitle
%\vspace*{-.6cm}
\begin{abstract}
%{\bf Abstract.}
\Abstract
%\vspace*{-.4cm}
\end{abstract}
\thispagestyle{empty}
\section{Introduction}\label{introduction}
%: introducing sparseness
The neural architecture is a complex dynamic system that operates at different time scales. In particular, one of its properties is to succeed in the feat of being able to both represent information quickly but also to be able to adapt in the long term autonomously (self-organized) to optimize this coding. In the case of the mammalian primary visual cortex (V1) for instance, one can observe the rapid coding of retinal image, as a process of transforming the visual information into a rough sketch that represents the outlines of objects in the image. This rapid operation, of the order of 50 milli-seconds in humans, is key to the results of~\textcite{hubel1968receptive}, who showed that some cells of the mammalian primary visual cortex have relatively localized receptive fields which are predominantly selective at different orientations. A major step in understanding this observation has been to show that the emergence of these filters can be explained as the coupling of a simple Hebbian learning with an ``optimal'' coding of the image. Indeed, the work of~\textcite{olshausen1996emergence} has shown that by imposing a sparse encoding of the image, we can obtain the emergence of such cells in a neural-type model. This type of unsupervised learning algorithm~\citep{Olshausen97} has been related to many other types of optimal representation algorithms used in both signal processing and artificial intelligence. In particular, this algorithm allows the extraction of the independent components of the signal, for example the orientations in the image. This representation makes it possible to observe the emergence of representations which are invariant to geometric transforms such as rotations and translations in V1. A recent rapprochement between these algorithms and machine learning algorithms makes it possible to place them in a new perspective. Indeed, these unsupervised algorithms are equivalent to a gradient descent optimization over an informational-type coding cost (Elad, VAE). This cost makes it possible to quantitatively evaluate the exploration of new components in the signal for learning with respect to the exploitation of these components in the coding. As such, this remark shows us that unsupervised learning consists of two antagonistic mechanisms, a long time scale that corresponds to the learning and exploration of new components and a faster scale that corresponds to coding.

%: in machine learning
It is important to note at this point that learning algorithms used in artificial intelligence can shed new light on the functioning of these neural processes and in particular on unsupervised learning. In the field of machine learning, unsupervised learning corresponds to learning representation dictionaries when the categorization data is unknown. This training is therefore carried out autonomously and is particularly used for image and signal compression, object detection, source separation and noise reduction in raw signals. As such, this class of algorithms is extremely useful in the first layers of artificial intelligence algorithms such as deep learning algorithms. There are many variants of such algorithms that take the form of either an optimization of information transfer (Bell), or in the form of recursive algorithms in statistics and probabilities with, for example, projection continuation. An important class of these algorithms considers that all the solutions that will be considered are those that correspond to an optimal coding. These solutions take the form of parsimonious coding, i. e. for which a small number of components will be selected according to the size of the dictionary. This principle is general enough to be applied to many signal classes and allow a mathematical analysis of this problem (Donoho). In particular, we have shown above that a rapid algorithm of parsimonious coding can be implemented in a normal neural architecture~\citep{Perrinet10shl}. It has also been shown that such coding can improve classification algorithms, in particular by limiting the number of layers required in a deep learning algorithm for classifying images that contain or do not contain animals~\citep{PerrinetBednar15}. However, recent studies seem to question this principle of parsimonious coding (Eichhorn 2009, Zoran and Weiss 2012) and suggest that simpler analysis applied in a complex metric is sufficient to explain the emergence of selective filters with orientation similar to those observed in the primary visual cortex.

%: encompassed in probabilistic approaches
In order to offer a broader perspective on this problem, we will try to express it in the generic form of a probabilistic problem. This approach is already widely used in Barlow's early work under the term of redundancy reduction principle (see also Atick). It led to translating this learning problem into a problem of efficient coding, for example by implementing inhibition rules in the retinal receptive field of the saber-toothed tiger (??? Srinivasan, 1981). Other studies show that these rules are tantamount to forcing the system to be close to a criticality regime and optimizing the balance between coding and learning (Beggo, 2008 in Sandin) More generally, we will place ourselves in the framework of the principle of free energy minimization formulated by Karl Friston. This principle makes it possible to explicitly address the problem of coding and unsupervised learning, also during learning. In this theory, learning is no longer a goal in itself but contributes to the the minimization of free energy at different time scales, from coding to learning. According to this principle, the overall goal of neural system is to be able to best predict any sensory input. This principle results in changes in the structure of the population (synaptic connections) but also in adaptation rule before the convergence of the learning. The goal of these processes is thus simply to not be surprised by the sensory input. As such, the aim of learning is to obtain a coding that is the least variable a priori, that is to say, before having received the sensory information. On the one hand, this theory extends that of Olshausen and shows that overall, sparse coding also has a predictive part. Thus, this will also allow us to formulate a normative theory of the neural code at its different time scales. Thus, the set of processes at different time scales are thus considered as working synergestically and provide for a novel normative theory of coding in early sensory areas such as V1.

%: homeostasis
%------------------------------%
%: see Figure~\ref{fig:map}
\begin{figure}%[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .9\linewidth) node {\includegraphics[width=.75\linewidth]{ssc_nohomeo}};
%\draw [anchor=north west] (0, 0) node {\includegraphics[width=.75\linewidth]{ssc_homeo}};
%4_D-comparing_convergence_results.pdf
\draw (.0\linewidth, .381\linewidth) node [above right=0mm] {$\mathsf{A}$};
\draw (.5\linewidth, .381\linewidth) node [above right=0mm] {$\mathsf{B}$};
\draw (.0\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.9\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Role of homeostasis in learning sparse representations}: 
We show the results of Sparse Hebbian Learning using  different homeostasis algorithms at convergence (20000 learning steps). For each algorithm, we show $30$ randomly drawn atoms from the $324$ filters of the same size as the image patches ($16 \times 16$) are presented in a matrix (separated by a white border). Note that their position in the matrix is arbitrary as in ICA. {\sf (A)} When switching off the cooperative homeostasis during learning, the corresponding Sparse Hebbian Learning algorithm converges to a set of filters that contains some less localized filters and some high-frequency Gabor functions that correspond to more ``textural'' features. One may wonder if these filters are inefficient and capturing noise or if they rather correspond to independent features of natural images in the LGM model. {\sf (B)} Results with the same coding and learning algorithm but by enabling homeostasis. % 
\label{fig:map}}%
\end{figure}%
%%------------------------------%
In particular, a component often ignored in this type of learning is the set of mechanisms of homeostasis. Indeed, these are implemented in the original algorithms (Olshausen, Rehn, \ldots{}) as a heuristic that simply prevents the algorithm from diverging. It consists in some recent algorithms to constitute only of an equalization of the energy of each component of the dictionary (Mairal). However, the neural mechanisms of homeostasis are at work in many components of the neural code and are essential to the overall functioning of the neural code. For example, the networks of GABA-type inhibitory neurons make it possible to regulate the overall activity of neural populations (citation needed). This mechanism then makes it possible to balance the contribution of the excitatory populations with respect to that in inhibitory populations. By this mechanism, this type of so-called balanced networks can explain many features inherent in the primary visual cortex (Hansel). These mechanisms, which often take the form of normalization rules (Schwartz) in the network, are often used as normative theory to explain the mechanisms present in the primary visual cortex (see Heeger or gain normalization by Simoncelli). However, this work is often intended to show that the cells that emerge from these algorithms have the same characteristics as that observed in neurophysiology (Rehn, Loxley). Other algorithms use nonlinearities that implicitly implement homeostatic rules in neuromimic algorithms (Gerstner and Brito). These non-linearities are mainly used in the output of successive layers of deep learning networks that are nowadays widely used for image classification or artificial intelligence. However most of these non-linear normalization rules are based on heuristics mimicking neural mechanisms but are not justified as part of the global problem of unsupervised learning. Framing this problem in a probabilistic framework allows to consider in addition to coding a	nd learning the intermediate time scale of homeostasis. This allows us also to associate homeostasis with adaptation mechanisms (Rao Balard). Our main argument is that by optimizing unsupervised learning at different time scales, we allow for the implementation of fast algorithms compatible with the performance of biological networks and in comparison with classical (Olshausen) or Deep Learning approaches.

%: outline
This paper is organized according to the following plan. First we will show the importance of homeostasis in unsupervised learning algorithms. We will derive an optimal rule for homeostatic adaptation based on histogram equalization. We will then show quantitative results of this optimal algorithm by applying it to different pairs of coding and learning algorithms. By using different learning databases, we will be able to give a quantitative analysis that will make it possible to compare these different solutions. To simplify the optimal rule of homeostasis, we will then deliver a neuro-mimetic homeostasis algorithm derived from the optimal rule by using a simple heuristic. We will then compare the results of this new algorithm with the optimal algorithm as well as with other existing unsupervised learning algorithms (Olshausen, Sandin). Moreover, by its nature, this algorithm can easily be extended to convolutional networks such as those used in deep learning neural networks. This extension is possible by extending the filter dictionary by the hypothesis of invariances to the translation of representations. Our results on different databases show the stable and rapid emergence of characteristic filters on these different bases. This result shows a probable prospect of extending this representation and for which we hope to obtain classification results superior to the algorithms existing in the state-of-the-art. Finally we will conclude by showing the importance of homeostasis in unsupervised learning algorithms.
% ----------------------------------------------------------------------
\section{Algorithms for unsupervised learning}\label{algorithm}
% ----------------------------------------------------------------------
%: free-energy formulation
Visual items composing natural images are often sparse, such that the brain may use this sparseness to reconstruct images with only a few set of these items. Given a set $\image = (\image_k)_{k=1}^L \in \RR^{L \times M} $ of $L$ signals $\image_k \in \RR^M$ (raveled images of $M$ pixels), dictionary learning aims at finding the best dictionary $\dico=(\dico_i)_{i=1}^N$ of $N$ atoms $\dico_i \in \RR^M$ such that they best represent the data. In probabilistic term, this amounts to minimize the (negative) free-energy $F$ as a bound on surprise:
$$ F = KL( p(\image ) || q(\image | \coef, \dico ) )$$
which is the Kullback-Leibler distance between the density of images according to a known generative model and the current approximation of this density using the current estimate of sparse coefficients $\coef = (\coef)_{k=1}^L \in \RR^{L \times N} $ and of the dictionary $\dico$.

Most of existing models of unsupervised learning aim at optimising a cost defined on prior assumptions on representation's sparseness. For instance, learning is accomplished in {\sc SparseNet}~\citep{Olshausen97} on patches taken from natural images as a sequence of coding and learning steps. First, knowing a dictionary of receptive fields $\dico_i$, the sparse coding is achieved using a gradient descent over a convex cost derived from a sparse prior probability distribution function of the coefficients $a_i$. Then, knowing this sparse solution, learning is defined as slowly changing the dictionary using Hebbian learning. In general, the parameterisation of the prior has major impacts on results of the sparse coding and thus on the emergence of edge-like receptive fields and requires proper tuning. In fact, the definition of the prior corresponds to an objective sparseness and does not always fit to the observed probability distribution function of the coefficients. 
In particular, using a known coding of the image $\coef$, we may approximate the probability using
$$ p(\image | \dico ) = \int  p(\image | \coef, \dico ) dP(\coef) \approx q = p(\image | \hat{\coef}, \dico ) P(\hat{\coef}) $$
%$$\hat{\coef}= \Arg\Max $$
This is the classical formulation from Olshausen and Field but this ignores the approximation during learning (objective only valid at convergence) - a nice move would be to use free-energy as a bound on surprise - and that homeosatsis comes as a additional mechanism - restricts the set of possible dictionaries.

% - noise comes from independent mixings (Cournot) - inversely, knowing the mixes, we extract the signal
% - we wish to reduce noise as much as possible by having the least number of sources => sparse coding (inverse fof central limit theorem?)
% - a measure is that the Orientation Selectivity (OS) is highest - because we know that the structure of the world is like this...
% - goals = memory / separation / representation

% Link with VAE : https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/


%
%The idea of learning dictionaries to sparse code image patch was first
%proposed in:
%
%
%Olshausen BA, and Field DJ.,
%<http://www.nature.com/nature/journal/v381/n6583/abs/381607a0.html Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images.>
%Nature, 381: 607-609, 1996.
%~\citep{Perrinet10shl,Perrinet15bicv}. This is supporting the idea that an unsupervised learning algorithm based on sparse coding could be use to describe efficiently image processing in the primary visual cortex. Learning is accomplished in {\sc SparseNet}~\citep{Olshausen97} on patches taken from natural images as a sequence of coding and learning steps


% the value of the positive coefficient does not matter much : more bits on address than on quantification: form of independenace...

Assuming a Generative Linear Model, such that $\image = \dico \cdot \coef + \epsilon $, where $\epsilon  \in \RR^{L \times M}$ is without loss of generality a Normal iid noise.  The sparse coding is obtained by minimizing a $\ell^0$ constrained optimization
$$ \umin{ \norm{x}_0 \leq N_0 } \frac{1}{2}\norm{\image - \dico \coef}^2 .  $$
where the $\ell^0$ pseudo-norm of $\coef_i \in \RR^N$ is
$$ \norm{\coef_k}_0 = \abs{\enscond{i}{\coef_k(i) \neq 0}}. $$
The parameter $N_0>0$ controls the amount of sparsity that we impose to the coding.

\begin{equation}%
\mathcal{C}_0( \coef | \image , \dico) = \frac{1}{2\sigma_n^2} \| \image - \dico \coef \|^2 + \lambda \| \coef \|_0 \nonumber%
\end{equation}%

\subsection{Algorithm: Sparse Coding}

It was found that by using an algorithm like Matching Pursuit, the learning algorithm could provide results similar to {\sc SparseNet}, but without the need of parametric assumptions on the prior~\citep{Perrinet10shl} by using this more generic $\ell_0$ norm sparseness. However, we observed that this class of algorithms could lead to solutions corresponding to a local minimum of the objective function: Some solutions seem as efficient as others for representing the signal but do not represent edge-like features homogeneously. In particular, during the early learning phase, some cells may learn ``faster'' than others. There is the need for a homeostasis mechanism that will ensure convergence of learning. The goal of this work is to study the specific role of homeostasis in learning sparse representations and to propose a homeostasis mechanism which optimises the learning of an efficient neural representation.%

\subsection{Algorithm: Sparse Hebbian Learning}

In particular, this could be a problem \emph{during} learning if we use the cost to measure representation efficiency for this learning step. An alternative is to 
perform a gradient descent on $\log[ F ] $:
$$ \frac{\partial }{\partial \dico_i } F = \frac{-1}{2 \sigma^2} \frac{\partial }{\partial \dico_i }[(\image - \dico \coef)^T (\image - \dico \coef)] = \frac{-1}{2 \sigma^2} \hat{\coef} (\image - \dico \hat{\coef})].$$
It is linear (at the single neuraon level) but the sparse code is not linear (at population level) - link with back-propagation


Dictionary learning performs an optimization both on the dictionary $D$
and the set of coefficients $ X = (x_j)_{j=1}^m \in \RR^{p \times m} $
where, for $j=1,\ldots,m$, $ x_j $
is the set of coefficients of the data $y_j$. This joint optimization reads
$$ \umin{ D \in \Dd, X \in \Xx_k } E(X,D) = \frac{1}{2}\norm{Y-DX}^2 =
\frac{1}{2} \sum_{j=1}^m \norm{y_j - D x_j}^2. $$


link of sparse coding with the cortical architecture (see rao and ballard) : 
- encoder / decoder
- variational auto-encoder
- sensory error versus prediction error

\subsection{Algorithm: Optimal homeostasis}\label{HEH}

% we know that during learning, there are distortions in the code / dictionary : normalisation is not enough.
During the early learning phase, some cells may learn ``faster'' than others. There is the need for a homeostasis mechanism that will ensure convergence of learning. The goal of this work is to study the specific role of homeostasis in learning sparse representations and to propose a homeostasis mechanism which optimises the learning of an efficient neural representation. 
% method zero = just normalizing the coefficitents (Mairal) -  first method = Olshausen's homeostasis that is a gradient descent on the variance of coefficients. serves as a control

To achieve this, we first formulate analytically the problem of representation efficiency in a population of sensory neurones. For the particular $\ell_0$ norm sparseness, we show that sparseness is optimal, in term of Shannon entropy, when average activity within the neural population is uniformly balanced (i.e. each neurone is selected with the same probability when encoding a large set of data). To achieve this uniformity, we define an homeostatic gain control mechanism based on histogram equalisation, that is in transforming coefficients in terms of z-scores $z_i(a_i) = P( \cdot > a_i)$. The cumulative distribution $z_i$ for each coefficient of the sparse vector is calculated using Hebbian learning to smooth its evolution during learning. At the coding level, this z-score function is incorporated in the matching step of the matching pursuit algorithm, to modulate the choice of the most  as that with the maximal z-score: $i^\ast = \mathrm{Argmax}_i z_i(a_i)$. The rest of the algorithm is left unchanged.

% using cumulative distribution = "inverse transform sampling" / a monotonic point scalar function does not change the LK distance / free-energy
In particular, we will set the a priori probability of selecting coefficients {\color{BrickRed} $\forall (i,j), P(\coef_i)=P(\coef_j)$} to ensure the optimality of the choice of the  pseudo $\ell_0$-norm and compare it to the representation in the primary visual cortex.%


\subsection{Computational implementation}
Finally, the unsupervised learning can be summarized using the following steps:
The proposed algorithm is:
\begin{enumerate}%
{\color{MidnightBlue} 
\item Initialize the point non-linear gain functions $z_i$ to similar cumulative distribution functions and the components $\dico_i$ to random points on the unit $L$-dimensional sphere,%
\item repeat until learning converged:%
\begin{enumerate}%
{\color{OliveGreen}
	\item draw a signal $\image$ from the database, its energy is $E = \| \image \|^2$,%
	\item set sparse vector $\coef$ to zero, initialize $\bar{a}_i=<\image, \dico_i >$ for all $i$,% using~\seeEq{coco},%
	\item while the residual energy $E$ is above a given threshold do:
	\begin{enumerate}
		{\color{BrickRed}
			\item select the best match: $i^\ast = \mbox{ArgMax}_{i} [z_i( \bar{a}_i )]$,% with~\seeEq{mp1},
		}
		\item set the sparse coefficient: $a_{i^\ast} = \bar{a}_{i^\ast}$,
		\item update residual coefficients: $\forall i, \bar{a}_i \leftarrow \bar{a}_i - a_{i^\ast} <\dico_{i^\ast} , \dico_i > $,% for all $i$ using~\seeEq{mp3},
		\item update energy: $E \leftarrow E - a_{i^\ast}^2 $.
	\end{enumerate}
}
\item when we have the sparse representation vector $\coef$, apply $\forall i$:
\begin{enumerate}
\item modify dictionary: $\dico_{i} \leftarrow \dico_{i} + \eta a_{i} (\image - \dico\coef)$,% using~\seeEq{learn}, 
\item normalize dictionary: $\dico_{i} \leftarrow \dico_{i} / \| \dico_{i}\|$,% using~\seeEq{learn}, 
{\color{BrickRed}
\item update homeostasis functions: $z_i( \cdot ) \leftarrow (1- \eta_h ) z_i( \cdot ) + \eta_h \delta( a_i \leq \cdot)$.% using~\seeEq{learn_homeo}
}
\end{enumerate}
\end{enumerate}
}
\end{enumerate}

%: scripts on github
This algorithm was implemented using python and the numpy library. In particular, we focused in our architecture to be able to test for zillions of parameters. Indeed, we think it is better to spend time on this than to have only one model with zillions of parameters that is optimized once.
Using the implemented sparse coding algorithms part of the sklearn library, we could control the convergence of the learning with different sparse coding algorithms. In particular we compared the learning as implemented with matching pursuit to that with  OMP, LARS or coordinate descent. 

\subsection{Quantitative Evaluation of dictionaries}
%%------------------------------%
%\begin{figure}[!hb]%[!ht]%%[p!]
%\centering{
%\begin{tikzpicture}
%\draw [anchor=north west] (.05\linewidth, .435\linewidth) node {\includegraphics[height=.435\linewidth]{figures/dictionary_example.pdf}};
%\draw [anchor=north west] (.5\linewidth, .45\linewidth) node {\includegraphics[width=.4\linewidth]{figures/angular_tuning.pdf}};
%\draw [anchor=north west] (.5\linewidth, .2\linewidth) node {\includegraphics[width=.4\linewidth]{figures/frequency_tuning.pdf}};
%\draw (0, .4\linewidth) node [above right=0mm] {$\mathsf{A}$};
%\draw (.45\linewidth, .4\linewidth) node [above right=0mm] {$\mathsf{B}$};
%\draw (.45\linewidth, .2\linewidth) node [above right=0mm] {$\mathsf{C}$};
%\end{tikzpicture}}
%\caption{
%{\bf  Receptive fields for increasing number of population cells}: 
%%
%We show in ($\mathsf{A}$) some cells drawn at random from populations of increasing sizes $M$ after the convergence ($20000$ learning steps) of an unsupervised learning algorithm. The algorithm was trained on $32\times 32$ patches drawn from natural images which were selected and pre-processed in a similar way as in the {\sc SparseNet} algorithm~\citep{olshausen1996emergence}. 
%%A first qualitative result is that learning converged for all different population sizes  to localized features within the patches. 
%Qualitatively, cells' selectivity resembled that present in the primary visual cortex but differed from the population with the lowest complexity to the  bigger populations.
%%
%%\label{fig:map}
%{\bf Quantitative analysis of the different populations of cells}: Using the response of cells in the populations to gratings of different orientations and spatial frequency, we show the ($\mathsf{B}$) orientation and ($\mathsf{C}$) frequency selectivity as measured by the standard deviation of tuning curves. While orientation tuning saturates around $M=400$, frequency tuning gets finer for larger populations ($M > 800$). %  of increasing sizes
%}%
%\end{figure}%
%%%------------------------------%

We compared qualitatively the set $\dico$ of receptive filters generated by the proposed algorithm when the homeostasis is first turned-off and then enabled  (see Fig.~\ref{fig:map}). A more quantitative study of the coding is shown by comparing selection distribution of sparse coefficients when the homeostasis mechanism is turned on (see Fig.~\ref{fig:quant}). We demonstrate that forcing the learning activity to be uniformly spread among all receptive fields results in a faster convergence of the representation error, and in an increase of the Shanon entropy. 
%------------------------------%
%: see Figure~\ref{fig:HEH}
\begin{figure}[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .8\linewidth) node {\includegraphics[width=\linewidth]{PDF_nohomeo}};
%\draw [anchor=north west] (0, .4\linewidth) node {\includegraphics[width=\linewidth]{z_score}};
%\draw [anchor=north west] (0, .0) node {\includegraphics[width=\linewidth]{PDF_homeo}};
\draw (.7\linewidth, 0) node [above right=0mm] {$\mathsf{A}$};
\draw (.3\linewidth, 0) node [above right=0mm] {$\mathsf{B}$};
\draw (-.1\linewidth, 0) node [above right=0mm] {$\mathsf{C}$};
\draw (.0\linewidth, .38\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.85\linewidth, .38\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Quantitative role of homeostasis in sparse coding}: We show the results of Sparse Coding using the two different homeostasis algorithms using surrogate data where each filter was equiprobable but for which we manipulated the first half of the coefficients to be artificially twice as big. %
{\sf (A)}~Such a situation replicates a situation arising during learning when a sub-group of filters is more active, e.~g. because it learned more salient features.  Here, we show the probability of the selection of the different filters (normalized to an average of $1$) which shows a bias of the standard Matching Pursuit to select more often filters whose activity is higher. %We evaluated the efficiency of retrieving the correct coefficients to about $\ %
{\sf (B)}~Non-linear homeostatic functions learned using Hebbian learning. These functions were initialized as the cumulative distribution function of uniform random variables. Then they are used to modify choices in the Matching step of the Matching Pursuit algorithm. Progressively, the non-linear functions converge to the (hidden) cumulative distributions of the coefficients of the surrogate, clearly showing the group of filters with twice a big coefficients. 
 {\sf (C)}~At convergence, the probability of choosing any filter is uniform. As a result, entropy is maximal, a property which is essential for the optimal representation of signals in distributed networks such as the brain.
\label{fig:HEH}}%
\end{figure}%
%%------------------------------%


% LUP\ IS\ HERE\ 



\section{Results: Fast unsupervised learning using homeostasis}\label{results}
%------------------------------%
%: see Figure~\ref{fig:HAP}
\begin{figure}[!ht]%%[p!]
\centering{
\begin{tikzpicture}
%\draw [anchor=north west] (0, .39\linewidth) node {\includegraphics[width=.47\linewidth]{dico_MP}};
%\draw [anchor=north west] (.5\linewidth, .39\linewidth) node {\includegraphics[width=.47\linewidth]{dico_SN}};
%\draw [anchor=north west] (.25\linewidth, -.1\linewidth) node {\includegraphics[width=.47\linewidth]{dico_MEUL}};
%\draw [anchor=north west] (.1\linewidth, -.6\linewidth) node {\includegraphics[width=.8\linewidth]{Comparison_reconstruction}};
\draw (0, .39\linewidth) node [above right=0mm] {$\mathsf{A}$};
\draw (.5\linewidth, .39\linewidth) node [above right=0mm] {$\mathsf{B}$};
%\draw (.2\linewidth, -.16\linewidth) node [above right=0mm] {$\mathsf{C}$};
%\draw (.05\linewidth, -.67\linewidth) node [above right=0mm] {$\mathsf{D}$};
\draw (.0\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\draw (.85\linewidth, .0\linewidth) node [above right=0mm] {$\mathsf{box}$};
\end{tikzpicture}}
\caption{
{\bf Quantitative evaluation of fast unsupervised learning}: We used the HAP approximation. %
 {\sf (A)}~30 from the 324 dictionaries learned {\sf (B)}~Comparison of the reconstruction error (computed as the square root of the squared difference between the image and the residual) for the 3 algorithms (HEH, SPARSENET, HAP): The convergence velocity of MEUL is higher than SPARSENET and MP.
\label{fig:HAP}}%
%\caption{
%{\bf Quantitative role of homeostasis in a classification network}: We used the generic MNIST protocol to assess the role of the homeostasis algorithm on classification. %
% {\sf (A-C)}~144 dictionaries learned from the MNIST database with a sparseness of 5 after 10000 iterations with {\sf (A)}~MP Algorithm ($\eta=0.01$): No homeostasis regulation, only a small subset of dictionaries are selected with a high probability to describe the dataset.
%{\sf (B)}~SPARSENET Algorithm ($\eta=0.01$, $\eta_h=0.01$, $\alpha_h=0.02$): The homeostasis regulation is made by normalizing the volatility.
%{\sf (C)}~MEUL Algorithm ($\eta=0.01$, $\eta_h=0.01$): All dictionaries are selected with the same probability to describe the dataset, leading to a cooperative learning.
% {\sf (D)}~Comparison of the reconstruction error (computed as the square root of the squared difference between the image and the residual) for the 3 algorithms (MEUL, SPARSENET, MP): The convergence velocity of MEUL is higher than SPARSENET and MP.
%\label{fig:quant}}%
\end{figure}%
%%------------------------------%

\subsection{Algorithm: Approximate homeostasis}\label{HAP}
% incompatible with nueromimetic / fast implementation
We have shown above that we can find an exact solution to the problem of homeostasis during sparse hebbian learning. However, this solution has several drawbacks. First, it is computationaly intesive on a conventional computer as it necessitates to store the cumulative distribution of each coefficient. More importantly, it seems that biological neurons seem to rather use a simple gain control mechanism. This is similar to that used by Olshausen. This is similar to the mechanisms of gain normalization proposed by Schwartz and which were recently shown to provide efficient coding mechanisms by Simoncelli. However, compared to these methods which manipulate the gain of dictionaries based on the energy of coeeficients, we propose to rather use a methodology based on the probability of activation.

% Comparison to Sandin
Recently, a similar approach was proposed by Sandin. Based on the same observations, authors propose to optimise the coding during learning by modulating the gain of each dictionarry element based on the recent 
 / correlation but now on the activation probability

% derivation p.56N de 2018-01)09 HULK

\subsection{Approximate homeostasis yields similar performance}



\subsection{Robustness of results}

% effect of coding algorithm

% effect of eta, eta_homeo

%: structural complexity: effect of size fo dictionary, 

\section{Discussion and conclusion}\label{discussion-et-conclusion}

% Plausibility of such a mechanism (see Laughlin et al)

%: 2- benchmarking of computation time: toward event-driven?

% Finally, we show that such an algorithm can be extended to convolutional architectures and we show the results obtained on different natural image databases. 
%
%One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any sample as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that sparse coding can be optimized by designing a proper homeostatic rule regulating the competition between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalisation similar to what is observed in biological neural networks. We validated this theoretical insight by challenging the matching pursuit sparse coding algorithm with the same learning rule but with or without homeostasis. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding accuracy did not vary much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we will apply this algorithm to the optimisation of a visual system to be embedded in an aerial robot. In summary, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms. 
 %The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. 

%2. importance of homeo - vanishing term in deep learning -> use deep learning to validate output
%3. application to asynchronous / focal  log-polar (retinal) input / continuous learning / credit assignement (no access to true residual)

%To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. Indeed, such robots have an increasing demand for visual applications, such as computation of optic flow for flight stabilization or for common visual categorization tasks such as recognition of pilot's gestures. However, embedding such a system is often difficult as it necessitates to be rapid,  energy efficient and lightweight. We resolved this problem by introducing our novel algorithm as an output of our visual sensors (the ``retina''). Results were collected by comparing the image reconstruction error for different information bandwidths in the output bus of this retina.Quantitative results show that the quality of the reconstruction was improved in our algorithm on a  database of natural, flying videos of the aerial robot.In particular, this algorithm provides with a real-life example showing how to improve our understanding of the emergence of edge-selective simple cells, drawing the bridge between structure (representation) and function (efficient coding).% 
%


 By developing this
fast learning algorithm we hope for its rapid application in artificial
intelligence algorithms. This type of architecture is economical,
efficient and fast. It makes it possible to be transferred to most deep
learning algorithms, a major flaw of which is to be very greedy in
computing resources. In particular, we are considering perspectives for
coding within a dynamic flow of sensory data and we hope to apply this
type of algorithm on embedded systems such as aerial robots. Along with
this, we hope that this new type of rapid unsupervised learning
algorithm can provide a normative theory for the coding of information
in low-level sensory processing, whether it is visual or auditory, for
example.


%\FigureMap
%\columnbreak
%tata
%\Figure
%\FigureQuant

%%%%%----------------------------------------------------------------- 
%%\section*{Acknowledgments}
%%%{\bf Acknowledgments}\\
%%\Acknowledgments

%%%----------------------------------------------------------------- 
%{\bf References} \\
\printbibliography
%Column 1
%\columnbreak
%Column 2

\end{document}